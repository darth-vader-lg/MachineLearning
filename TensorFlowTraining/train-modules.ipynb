{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oGKniD-fesc"
      },
      "source": [
        "# Object detection train with TensorFlow 2.4.1\r\n",
        "This notebook will train a model for the object detection purpouse.\r\n",
        "It can be run as a Jupiter notebook in the Google Colab environment or exported as a Python file and run from a command line.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_J9UyxqQpAR"
      },
      "source": [
        "#Model types\r\n",
        "Initialize the list of the available pre-trained models and their parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV1W9gDHQcjf"
      },
      "source": [
        "\"\"\" List of the available models and their definitions \"\"\"\r\n",
        "models = {\r\n",
        "    'SSD MobileNet v2 320x320': {\r\n",
        "        'DownloadPath': 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\r\n",
        "        'batch_size': 12,\r\n",
        "        'height': 300,\r\n",
        "        'width': 300\r\n",
        "    },\r\n",
        "    'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)': {\r\n",
        "        'DownloadPath': 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz',\r\n",
        "        'batch_size': 8,\r\n",
        "        'height': 640,\r\n",
        "        'width': 640\r\n",
        "    },\r\n",
        "}\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    import pprint\r\n",
        "    pprint.PrettyPrinter(1).pprint(models)\r\n",
        "    print('Dictionary of pre-trained models configured')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68KXcQaTRMeN"
      },
      "source": [
        "#Base train parameters\r\n",
        "**model_type:**\r\n",
        "\r\n",
        ">The chosen base pre-trained model type (from the list in the previous cell). Write it between quotation marks.\r\n",
        "\r\n",
        "**model_dir:**\r\n",
        "> the output directry for the trained models' checkpoints.\r\n",
        "\r\n",
        "**train_images_dir:**\r\n",
        "> the directory on your Google Drive containing the images for the train and their xml annotations. You can use standard images annotation tools as [labelImg](https://github.com/tzutalin/labelImg), [tVoTT](https://github.com/microsoft/VoTT), etc... for the annotation task.\r\n",
        "Put the xml generated files in te same directory of the images.\r\n",
        "\r\n",
        "**eval_images_dir:**\r\n",
        "> the directory on your Google Drive containing the images used for evaluating the train. They could be near from 10% to 20% of the number of the train images. They must be labeled as the train images.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE1vVxUaQycS"
      },
      "source": [
        "import  os\r\n",
        "from    pathlib import Path\r\n",
        "import  tempfile\r\n",
        "import  sys\r\n",
        "\r\n",
        "class BaseParameters:\r\n",
        "    \"\"\" Class holding the base parameters \"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        \"\"\" Constructor \"\"\"\r\n",
        "        self._flags = flags.FLAGS\r\n",
        "        self._flags.mark_as_parsed()\r\n",
        "        self._model_type = 'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)'\r\n",
        "        self._model_dir = 'trained-model'\r\n",
        "        self._train_images_dir = 'images/train'\r\n",
        "        self._eval_images_dir = 'images/eval'\r\n",
        "    @property\r\n",
        "    def model_type(self): return self._flags.model_type or self._model_type\r\n",
        "    @property\r\n",
        "    def model_dir(self): return self._flags.model_dir or self._model_dir\r\n",
        "    @model_dir.setter\r\n",
        "    def model_dir(self, value): self._flags.model_dir = self._model_dir = value\r\n",
        "    @property\r\n",
        "    def train_images_dir(self): return self._flags.train_images_dir or self._train_images_dir\r\n",
        "    @train_images_dir.setter\r\n",
        "    def train_images_dir(self, value): self._flags.train_images_dir = self._train_images_dir = value\r\n",
        "    @property\r\n",
        "    def eval_images_dir(self): return self._flags.eval_images_dir or self._eval_images_dir\r\n",
        "    @eval_images_dir.setter\r\n",
        "    def eval_images_dir(self, value): self._flags.eval_images_dir = self._eval_images_dir = value\r\n",
        "    @property\r\n",
        "    def annotations_dir(self): return 'annotations'\r\n",
        "    @property\r\n",
        "    def model(self):\r\n",
        "        global models\r\n",
        "        return models[self.model_type]\r\n",
        "    @property\r\n",
        "    def pre_trained_model_base_dir(self): return os.path.join(tempfile.gettempdir(), \"tensorflow-pre-trained-models\")\r\n",
        "    @property\r\n",
        "    def pre_trained_model_dir(self):\r\n",
        "        return str(Path(os.path.join(self.pre_trained_model_base_dir, Path(self.model[\"DownloadPath\"]).name)).with_suffix(\"\").with_suffix(\"\"))\r\n",
        "\r\n",
        "\"\"\" Arguments definition \"\"\"\r\n",
        "from absl import flags\r\n",
        "if (not 'model_type' in flags.FLAGS):\r\n",
        "    flags.DEFINE_string(\r\n",
        "        'model_type',\r\n",
        "        'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)',\r\n",
        "        'Type of the base model.')\r\n",
        "    flags.DEFINE_string(\r\n",
        "        'model_dir',\r\n",
        "        'trained-model',\r\n",
        "        'Path to output model directory where event and checkpoint files will be written.')\r\n",
        "    flags.DEFINE_string(\r\n",
        "        'train_images_dir',\r\n",
        "        'images/train',\r\n",
        "        'Path to the directory containing the images for train and their labeling xml.')\r\n",
        "    flags.DEFINE_string(\r\n",
        "        'eval_images_dir',\r\n",
        "        'images/eval',\r\n",
        "        'Path to the directory containing the images for evaluate and their labeling xml.')\r\n",
        "\r\n",
        "prm = BaseParameters()\r\n",
        "print(f'Model type: {prm.model_type}')\r\n",
        "print(f'Output directory: {str(Path(prm.model_dir).resolve())}')\r\n",
        "print(f'Train images directory: {str(Path(prm.train_images_dir).resolve())}')\r\n",
        "print(f'Evaluation images directory: {str(Path(prm.eval_images_dir).resolve())}')\r\n",
        "print('Base parameters configured')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSrVPdw5SRjb"
      },
      "source": [
        "#Train parameters\r\n",
        "A list of train parameters. Read the comments in the flags section for the usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eECHryVmTLg"
      },
      "source": [
        "import  sys\r\n",
        "\r\n",
        "class TrainParameters(BaseParameters):\r\n",
        "    \"\"\" Class holding the train execution parameters \"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        \"\"\" Constructor \"\"\"\r\n",
        "        super().__init__()\r\n",
        "        self._num_train_steps = 10000\r\n",
        "        self._eval_on_train_data = False\r\n",
        "        self._sample_1_of_n_eval_examples = None\r\n",
        "        self._sample_1_of_n_eval_on_train_examples = 5\r\n",
        "        self._checkpoint_dir = None\r\n",
        "        self._eval_timeout = 3600\r\n",
        "        self._use_tpu = False\r\n",
        "        self._tpu_name = None\r\n",
        "        self._num_workers = 1\r\n",
        "        self._checkpoint_every_n = 1000\r\n",
        "        self._record_summaries = True\r\n",
        "    @property\r\n",
        "    def num_train_steps(self):\r\n",
        "        return self._flags.num_train_steps or self._num_train_steps\r\n",
        "    @property\r\n",
        "    def eval_on_train_data(self):\r\n",
        "        return self._flags.eval_on_train_data or self._eval_on_train_data\r\n",
        "    @property\r\n",
        "    def sample_1_of_n_eval_examples(self):\r\n",
        "        return self._flags.sample_1_of_n_eval_examples or self._sample_1_of_n_eval_examples\r\n",
        "    @property\r\n",
        "    def sample_1_of_n_eval_on_train_examples(self):\r\n",
        "        return self._flags.sample_1_of_n_eval_on_train_examples or self._sample_1_of_n_eval_on_train_examples\r\n",
        "    @property\r\n",
        "    def checkpoint_dir(self):\r\n",
        "        return self._flags.checkpoint_dir or self._checkpoint_dir\r\n",
        "    @property\r\n",
        "    def eval_timeout(self):\r\n",
        "        return self._flags.eval_timeout or self._eval_timeout\r\n",
        "    @property\r\n",
        "    def use_tpu(self):\r\n",
        "        return self._flags.use_tpu or self._use_tpu\r\n",
        "    @property\r\n",
        "    def tpu_name(self):\r\n",
        "        return self._flags.tpu_name or self._tpu_name\r\n",
        "    @property\r\n",
        "    def num_workers(self):\r\n",
        "        return self._flags.num_workers or self._num_workers\r\n",
        "    @property\r\n",
        "    def checkpoint_every_n(self):\r\n",
        "        return self._flags.checkpoint_every_n or self._checkpoint_every_n\r\n",
        "    @property\r\n",
        "    def record_summaries(self):\r\n",
        "        return self._flags.record_summaries or self._record_summaries\r\n",
        "\r\n",
        "\"\"\" Arguments definition \"\"\"\r\n",
        "from absl import flags\r\n",
        "if (not 'num_train_steps' in flags.FLAGS):\r\n",
        "    flags.DEFINE_integer(\r\n",
        "        'num_train_steps',\r\n",
        "        10000,\r\n",
        "        'Number of train steps.')\r\n",
        "    flags.DEFINE_bool(\r\n",
        "        'eval_on_train_data',\r\n",
        "        False,\r\n",
        "        'Enable evaluating on train data (only supported in distributed training).')\r\n",
        "    flags.DEFINE_integer(\r\n",
        "        'sample_1_of_n_eval_examples',\r\n",
        "        None,\r\n",
        "        'Will sample one of every n eval input examples, where n is provided.')\r\n",
        "    flags.DEFINE_integer(\r\n",
        "        'sample_1_of_n_eval_on_train_examples',\r\n",
        "        5,\r\n",
        "        'Will sample one of every n train input examples for evaluation, '\r\n",
        "        'where n is provided. This is only used if `eval_training_data` is True.')\r\n",
        "    flags.DEFINE_string(\r\n",
        "        'checkpoint_dir',\r\n",
        "        None,\r\n",
        "        'Path to directory holding a checkpoint. If `checkpoint_dir` is provided, '\r\n",
        "        'this binary operates in eval-only mode, writing resulting metrics to `model_dir`.')\r\n",
        "    flags.DEFINE_integer(\r\n",
        "        'eval_timeout',\r\n",
        "        3600,\r\n",
        "        'Number of seconds to wait for an evaluation checkpoint before exiting.')\r\n",
        "    flags.DEFINE_bool(\r\n",
        "        'use_tpu',\r\n",
        "        False,\r\n",
        "        'Whether the job is executing on a TPU.')\r\n",
        "    flags.DEFINE_string(\r\n",
        "        'tpu_name',\r\n",
        "        None,\r\n",
        "        'Name of the Cloud TPU for Cluster Resolvers.')\r\n",
        "    flags.DEFINE_integer(\r\n",
        "        'num_workers',\r\n",
        "        1,\r\n",
        "        'When num_workers > 1, training uses MultiWorkerMirroredStrategy. '\r\n",
        "        'When num_workers = 1 it uses MirroredStrategy.')\r\n",
        "    flags.DEFINE_integer(\r\n",
        "        'checkpoint_every_n',\r\n",
        "        1000,\r\n",
        "        'Integer defining how often we checkpoint.')\r\n",
        "    flags.DEFINE_boolean(\r\n",
        "        'record_summaries',\r\n",
        "        True,\r\n",
        "        'Whether or not to record summaries during training.')\r\n",
        "\r\n",
        "prm = TrainParameters()\r\n",
        "\r\n",
        "print(f'Number of train steps: {prm.num_train_steps}')\r\n",
        "print(f'Save a checkpoint every {prm.checkpoint_every_n} steps')\r\n",
        "print('Train parameters configured')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH7-qjJ6UMH-"
      },
      "source": [
        "#Utility functions\r\n",
        "Some utility functions used for the steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zImaNt8dTqRj"
      },
      "source": [
        "import subprocess\r\n",
        "import sys\r\n",
        "\r\n",
        "def execute_subprocess(cmd):\r\n",
        "    \"\"\"\r\n",
        "    Execute a subprocess returning each line of the standard output.\r\n",
        "    Keyword arguments:\r\n",
        "    cmd     -- the process to execute with its parameters\r\n",
        "    \"\"\"\r\n",
        "    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, universal_newlines=True)\r\n",
        "    for stdout_line in iter(popen.stdout.readline, \"\"):\r\n",
        "        yield stdout_line \r\n",
        "    popen.stdout.close()\r\n",
        "    return_code = popen.wait()\r\n",
        "    if return_code:\r\n",
        "        raise subprocess.CalledProcessError(return_code, cmd)\r\n",
        "\r\n",
        "def execute(cmd):\r\n",
        "    \"\"\"\r\n",
        "    Execute a subprocess printing its standard output.\r\n",
        "    Keyword arguments:\r\n",
        "    cmd     -- the process to execute with its parameters\r\n",
        "    \"\"\"\r\n",
        "    for output in execute_subprocess(cmd):\r\n",
        "        print(output, end=\"\")\r\n",
        "\r\n",
        "def execute_colab(fn):\r\n",
        "    \"\"\"\r\n",
        "    Execute a function only in the Google Colab environment.\r\n",
        "    Keyword arguments:\r\n",
        "    fn      -- the function to execute\r\n",
        "    \"\"\"\r\n",
        "    if ('google.colab' in sys.modules):\r\n",
        "        fn()\r\n",
        "\r\n",
        "def execute_non_colab(fn):\r\n",
        "    \"\"\"\r\n",
        "    Execute a function only outside the Google Colab environment.\r\n",
        "    Keyword arguments:\r\n",
        "    fn      -- the function to execute\r\n",
        "    \"\"\"\r\n",
        "    if (not 'google.colab' in sys.modules):\r\n",
        "        fn()\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    print('Utilities functions initialized')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2J0grGvUeAr"
      },
      "source": [
        "#Object detection environment installation\r\n",
        "This step installs a well known Python environment for the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czw15dMATgWa"
      },
      "source": [
        "import os\r\n",
        "import datetime\r\n",
        "from   pathlib import Path\r\n",
        "import shutil\r\n",
        "import sys\r\n",
        "import tempfile\r\n",
        "\r\n",
        "def install_object_detection():\r\n",
        "    \"\"\"\r\n",
        "    Install a well known environment.\r\n",
        "    \"\"\"\r\n",
        "    # Path of the python interpreter executable\r\n",
        "    pythonPath = os.path.join(os.path.dirname(sys.executable), 'python3')\r\n",
        "    if (not os.path.exists(pythonPath)):\r\n",
        "        pythonPath = os.path.join(os.path.dirname(sys.executable), 'python')\r\n",
        "    # Upgrade pip and setuptools\r\n",
        "    execute([pythonPath, '-m', 'pip', 'install', '--upgrade', 'pip==21.0.1'])\r\n",
        "    execute([pythonPath, '-m', 'pip', 'install', '--upgrade', 'setuptools==54.0.0'])\r\n",
        "    # Install TensorFlow\r\n",
        "    execute([pythonPath, '-m', 'pip', 'install', 'tensorflow==2.4.1'])\r\n",
        "    # Install pygit2\r\n",
        "    execute([pythonPath, '-m', 'pip', 'install', 'pygit2==1.5.0'])\r\n",
        "    import pygit2\r\n",
        "    # Progress class for the git output\r\n",
        "    class GitCallbacks(pygit2.RemoteCallbacks):\r\n",
        "        def __init__(self, credentials=None, certificate=None):\r\n",
        "            self.dateTime = datetime.datetime.now()\r\n",
        "            return super().__init__(credentials=credentials, certificate=certificate)\r\n",
        "        def transfer_progress(self, stats):\r\n",
        "            now = datetime.datetime.now()\r\n",
        "            if ((now - self.dateTime).total_seconds() > 1):\r\n",
        "                print('\\rReceiving... Deltas [%d / %d], Objects [%d / %d]'%(stats.indexed_deltas, stats.total_deltas, stats.indexed_objects, stats.total_objects), end='', flush=True)\r\n",
        "                self.dateTime = now\r\n",
        "            if (stats.received_objects >= stats.total_objects and stats.indexed_objects >= stats.total_objects and stats.indexed_deltas >= stats.total_deltas):\r\n",
        "                print('\\r\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\rDone Deltas %d, Objects %d.'%(stats.total_objects, stats.total_objects))\r\n",
        "            return super().transfer_progress(stats)\r\n",
        "    # Directory of the TensorFlow object detection api\r\n",
        "    odApiDir = os.path.join(tempfile.gettempdir(), 'tensorflow-object-detection-api')\r\n",
        "    # Install the TensorFlow models\r\n",
        "    if (not os.path.isdir(odApiDir)):\r\n",
        "        # Create the callback for the progress\r\n",
        "        callbacks = GitCallbacks();\r\n",
        "        # Clone the TensorFlow models repository\r\n",
        "        print('Cloning the TensorFlow models repository')\r\n",
        "        pygit2.clone_repository('https://github.com/tensorflow/models.git', odApiDir, callbacks = callbacks)\r\n",
        "        print('TensorFlow models repository cloned')\r\n",
        "        # Checkout a well known commit\r\n",
        "        repo = pygit2.Repository(odApiDir)\r\n",
        "        ish = 'e356598a5b79a768942168b10d9c1acaa923bdb4'\r\n",
        "        (commit, reference) = repo.resolve_refish(ish)\r\n",
        "        repo.checkout_tree(commit)\r\n",
        "        repo.reset(pygit2.Oid(hex=ish), pygit2.GIT_RESET_HARD)\r\n",
        "        # Move to the research dir\r\n",
        "        currentDir = os.getcwd()\r\n",
        "        os.chdir(os.path.join(odApiDir, 'research'))\r\n",
        "        # Install the protobuf tools\r\n",
        "        execute([pythonPath, '-m', 'pip', 'install', 'grpcio-tools==1.32.0'])\r\n",
        "        # Compile the protobufs\r\n",
        "        import grpc_tools.protoc as protoc\r\n",
        "        protoFiles = Path('object_detection/protos').rglob('*.proto')\r\n",
        "        for protoFile in protoFiles:\r\n",
        "            protoFilePath = str(protoFile)\r\n",
        "            print('Compiling', protoFilePath)\r\n",
        "            protoc.main(['grpc_tools.protoc', '--python_out=.', protoFilePath])\r\n",
        "        # Install the object detection packages\r\n",
        "        shutil.copy2('object_detection/packages/tf2/setup.py', '.')\r\n",
        "        execute([pythonPath, '-m', 'pip', 'install', '.'])\r\n",
        "        os.chdir(currentDir)\r\n",
        "    sys.path.append(os.path.join(odApiDir, 'research'))\r\n",
        "    sys.path.append(os.path.join(odApiDir, 'research/slim'))\r\n",
        "    print('Installation completed.')\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    install_object_detection()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcve5HqPVaYS"
      },
      "source": [
        "# Environment initialization\r\n",
        "In this section the environmento for the training will be initialized.\r\n",
        "\r\n",
        "All necessary directories will be crated and the Google drive containing the images will be mounted. Follow the instruction for the mounting during the execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JzO6ORpVBkP"
      },
      "source": [
        "from    absl import app\r\n",
        "import  os\r\n",
        "from    pathlib import Path\r\n",
        "import  sys\r\n",
        "\r\n",
        "def init_train_environment(prm = TrainParameters()):\r\n",
        "    \"\"\"\r\n",
        "    Initialize the training environment with the right directories structure.\r\n",
        "    Keyword arguments:\r\n",
        "    prm     -- the train parameters\r\n",
        "    \"\"\"\r\n",
        "    # Set the configuration for Google Colab\r\n",
        "    if ('google.colab' in sys.modules):\r\n",
        "        if (not os.path.exists('/mnt/MyDrive')):\r\n",
        "            print('Mounting the GDrive')\r\n",
        "            from google.colab import drive\r\n",
        "            drive.mount('/mnt')\r\n",
        "        # Check the existence of the train images dir\r\n",
        "        gdriveOutputDir = os.path.join('/mnt', 'MyDrive', prm.train_images_dir)\r\n",
        "        if (not os.path.isdir(gdriveOutputDir)):\r\n",
        "            raise Exception('Error!!! The train images dir doesn`t exist')\r\n",
        "        if (os.path.exists('/content/train-images')):\r\n",
        "            os.unlink('/content/train-images')\r\n",
        "        os.symlink(gdriveOutputDir, '/content/train-images', True)\r\n",
        "        print(f\"Google drive's {prm.train_images_dir} is linked to /content/train-images\")\r\n",
        "        prm.train_images_dir = '/content/train-images'\r\n",
        "        # Check the existence of the evaluation images dir\r\n",
        "        gdriveOutputDir = os.path.join('/mnt', 'MyDrive', prm.eval_images_dir)\r\n",
        "        if (not os.path.isdir(gdriveOutputDir)):\r\n",
        "            raise Exception('Error!!! The evaluation images dir doesn`t exist')\r\n",
        "        if (os.path.exists('/content/eval-images')):\r\n",
        "            os.unlink('/content/eval-images')\r\n",
        "        os.symlink(gdriveOutputDir, '/content/eval-images', True)\r\n",
        "        print(f\"Google drive's {prm.eval_images_dir} is linked to /content/eval-images\")\r\n",
        "        prm.eval_images_dir = '/content/train-images'\r\n",
        "        # Check the existence of the output directory\r\n",
        "        gdriveOutputDir = os.path.join('/mnt', 'MyDrive', prm.model_dir)\r\n",
        "        if (not os.path.isdir(gdriveOutputDir)):\r\n",
        "            print('Creating the output directory')\r\n",
        "            os.mkdir(gdriveOutputDir)\r\n",
        "        if (os.path.exists('/content/trained-model')):\r\n",
        "            os.unlink('/content/trained-model')\r\n",
        "        os.symlink(gdriveOutputDir, '/content/trained-model', True)\r\n",
        "        print(f\"Google drive's {prm.model_dir} is linked to /content/trained-model\")\r\n",
        "        prm.model_dir = '/content/trained-model'\r\n",
        "    else:\r\n",
        "        if (not os.path.isdir(prm.train_images_dir)):\r\n",
        "            raise Exception('Error!!! The train images dir doesn`t exist')\r\n",
        "        print(f'Train images from {str(Path(prm.train_images_dir).resolve())}')\r\n",
        "        if (not os.path.isdir(prm.eval_images_dir)):\r\n",
        "            raise Exception('\"Error!!! The evaluation images dir doesn`t exist')\r\n",
        "        print(f'Train images from {str(Path(prm.eval_images_dir).resolve())}')\r\n",
        "        if (not os.path.exists(prm.model_dir)):\r\n",
        "            print('Creating the output dir')\r\n",
        "            os.mkdir(prm.model_dir)\r\n",
        "        print(f'The trained model will be in {str(Path(prm.model_dir).resolve())}')\r\n",
        "    if (not os.path.exists(prm.annotations_dir)):\r\n",
        "        os.mkdir(prm.annotations_dir)\r\n",
        "    print(f'The annotations files will be in {str(Path(prm.annotations_dir).resolve())}')\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    init_train_environment()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgqk5TdDdqid"
      },
      "source": [
        "#Pre-trained model download\r\n",
        "Dowloading the pre-trained model from the TensorFlow 2 model zoo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItFyPSm6dfgL"
      },
      "source": [
        "import  os\r\n",
        "from    pathlib import Path\r\n",
        "from    urllib import request\r\n",
        "\r\n",
        "def download_pretrained_model(prm = BaseParameters()):\r\n",
        "    \"\"\"\r\n",
        "    Download from the TensorFlow model zoo the pre-trained model.\r\n",
        "    Keyword arguments:\r\n",
        "    prm     -- the base parameters\r\n",
        "    \"\"\"\r\n",
        "    if (not os.path.exists(prm.pre_trained_model_dir) or not os.path.exists(os.path.join(prm.pre_trained_model_dir, 'checkpoint', 'checkpoint'))):\r\n",
        "        if (not os.path.exists(prm.pre_trained_model_base_dir)):\r\n",
        "            os.mkdir(prm.pre_trained_model_base_dir)\r\n",
        "        pre_trained_model_file = prm.pre_trained_model_dir + '.tar.gz'\r\n",
        "        print(f'Downloading the pre-trained model {str(Path(pre_trained_model_file).name)}...')\r\n",
        "        import tarfile\r\n",
        "        request.urlretrieve(prm.model['DownloadPath'], pre_trained_model_file) # TODO: show progress\r\n",
        "        print('Done.')\r\n",
        "        print(f'Extracting the pre-trained model {str(Path(pre_trained_model_file).name)}...')\r\n",
        "        tar = tarfile.open(pre_trained_model_file)\r\n",
        "        tar.extractall(prm.pre_trained_model_base_dir)\r\n",
        "        tar.close()\r\n",
        "        os.remove(pre_trained_model_file)\r\n",
        "        print(f'Pre-trained model is located at {str(Path(prm.pre_trained_model_dir).resolve())}')\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    download_pretrained_model()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCmXI3mkftFy"
      },
      "source": [
        "#TensorFlow's records\r\n",
        "In this step there will be created the TensorFlow records from the annotated images and the file contained all the labels' indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAqirnmAfq1P"
      },
      "source": [
        "import  glob\r\n",
        "import  io\r\n",
        "import  os\r\n",
        "from    pathlib import Path\r\n",
        "import  shutil\r\n",
        "\r\n",
        "class TFRecord:\r\n",
        "    \"\"\"Class for the TensorFlow records creation\"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        \"\"\" Constructor \"\"\"\r\n",
        "        super().__init__()\r\n",
        "        self._label_set = set()\r\n",
        "        self._label_dict = dict()\r\n",
        "    def class_text_to_int(self, row_label):\r\n",
        "        \"\"\"\r\n",
        "        Convertion of the text of the labels to an integer index\r\n",
        "        Keyword arguments:\r\n",
        "        row_label   -- the label to convert to int\r\n",
        "        \"\"\"\r\n",
        "        if (len(self._label_dict) == 0):\r\n",
        "            count = len(self._label_set)\r\n",
        "            labelIx = 1\r\n",
        "            for label in self._label_set:\r\n",
        "                self._label_dict[label] = labelIx\r\n",
        "                labelIx += 1\r\n",
        "        return self._label_dict[row_label]\r\n",
        "    def create_tf_example(self, group, path):\r\n",
        "        \"\"\"\r\n",
        "        TensorFlow example creator\r\n",
        "        Keyword arguments:\r\n",
        "        group   -- group's name\r\n",
        "        path    -- path of the labeled images\r\n",
        "        \"\"\"\r\n",
        "        from object_detection.utils import dataset_util\r\n",
        "        from PIL import Image\r\n",
        "        import tensorflow as tf\r\n",
        "        with tf.compat.v1.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\r\n",
        "            encoded_jpg = fid.read()\r\n",
        "        encoded_jpg_io = io.BytesIO(encoded_jpg)\r\n",
        "        image = Image.open(encoded_jpg_io)\r\n",
        "        width, height = image.size\r\n",
        "        filename = group.filename.encode('utf8')\r\n",
        "        image_format = b'jpg'\r\n",
        "        xmins = []\r\n",
        "        xmaxs = []\r\n",
        "        ymins = []\r\n",
        "        ymaxs = []\r\n",
        "        classes_text = []\r\n",
        "        classes = []\r\n",
        "        for index, row in group.object.iterrows():\r\n",
        "            xmins.append(row['xmin'] / width)\r\n",
        "            xmaxs.append(row['xmax'] / width)\r\n",
        "            ymins.append(row['ymin'] / height)\r\n",
        "            ymaxs.append(row['ymax'] / height)\r\n",
        "            classes_text.append(row['class'].encode('utf8'))\r\n",
        "            classes.append(self.class_text_to_int(row['class']))\r\n",
        "        tf_example = tf.train.Example(features=tf.train.Features(feature={\r\n",
        "            'image/height': dataset_util.int64_feature(height),\r\n",
        "            'image/width': dataset_util.int64_feature(width),\r\n",
        "            'image/filename': dataset_util.bytes_feature(filename),\r\n",
        "            'image/source_id': dataset_util.bytes_feature(filename),\r\n",
        "            'image/encoded': dataset_util.bytes_feature(encoded_jpg),\r\n",
        "            'image/format': dataset_util.bytes_feature(image_format),\r\n",
        "            'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\r\n",
        "            'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\r\n",
        "            'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\r\n",
        "            'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\r\n",
        "            'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\r\n",
        "            'image/object/class/label': dataset_util.int64_list_feature(classes),\r\n",
        "        }))\r\n",
        "        return tf_example\r\n",
        "    def create_tf_record(self, image_dir, output_file, labels_file = None, csv_file = None):\r\n",
        "        \"\"\"\r\n",
        "        TensorFlow record creator\r\n",
        "        Keyword arguments:\r\n",
        "        image_dir   -- the directory containing the images\r\n",
        "        output_file -- the output file path and name\r\n",
        "        labels_file -- the optional output file path and name of the resulting labels file\r\n",
        "        csv_file    -- the optional output file path and name of the csv file\r\n",
        "        \"\"\"\r\n",
        "        import tensorflow as tf\r\n",
        "        writer = tf.compat.v1.python_io.TFRecordWriter(output_file)\r\n",
        "        path = os.path.join(image_dir)\r\n",
        "        examples = self.xml_to_csv(image_dir)\r\n",
        "        grouped = self.split(examples, 'filename')\r\n",
        "        for group in grouped:\r\n",
        "            tf_example = self.create_tf_example(group, path)\r\n",
        "            writer.write(tf_example.SerializeToString())\r\n",
        "        writer.close()\r\n",
        "        print(f'Created the TFRecord file {str(Path(output_file).resolve())}')\r\n",
        "        if labels_file is not None:\r\n",
        "            from google.protobuf import text_format\r\n",
        "            from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\r\n",
        "            msg = StringIntLabelMap()\r\n",
        "            for id, name in enumerate(self._label_set, start = 1):\r\n",
        "                msg.item.append(StringIntLabelMapItem(id = id, name = name))\r\n",
        "            text = str(text_format.MessageToBytes(msg, as_utf8 = True), 'utf-8')\r\n",
        "            with open(labels_file, 'w') as f:\r\n",
        "                f.write(text)\r\n",
        "            print(f'Created the labels map file {str(Path(labels_file).resolve())}')\r\n",
        "        if csv_file is not None:\r\n",
        "            examples.to_csv(csv_file, index = None)\r\n",
        "            print(f'Created the CSV file {str(Path(csv_file).resolve())}')\r\n",
        "    def split(self, df, group):\r\n",
        "        \"\"\"\r\n",
        "        Split the labels in an image\r\n",
        "        Keyword arguments:\r\n",
        "        df      -- TensorFlow example\r\n",
        "        group   -- group's name\r\n",
        "        \"\"\"\r\n",
        "        from collections import namedtuple\r\n",
        "        data = namedtuple('data', ['filename', 'object'])\r\n",
        "        gb = df.groupby(group)\r\n",
        "        return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\r\n",
        "    def xml_to_csv(self, path):\r\n",
        "        \"\"\"\r\n",
        "        Convert the xml files generated by labeling image softwares into the cvs panda format\r\n",
        "        Keyword arguments:\r\n",
        "        path    -- Path of the generated csv file\r\n",
        "        \"\"\"\r\n",
        "        import pandas as pd\r\n",
        "        import xml.etree.ElementTree as ET\r\n",
        "        xml_list = []\r\n",
        "        for xml_file in glob.glob(path + '/*.xml'):\r\n",
        "            tree = ET.parse(xml_file)\r\n",
        "            root = tree.getroot()\r\n",
        "            for member in root.findall('object'):\r\n",
        "                value = (\r\n",
        "                    root.find('filename').text,\r\n",
        "                    int(root.find('size')[0].text),\r\n",
        "                    int(root.find('size')[1].text),\r\n",
        "                    member[0].text,\r\n",
        "                    int(member[4][0].text),\r\n",
        "                    int(member[4][1].text),\r\n",
        "                    int(member[4][2].text),\r\n",
        "                    int(member[4][3].text))\r\n",
        "                xml_list.append(value)\r\n",
        "                self._label_set.add(member[0].text)\r\n",
        "        column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\r\n",
        "        xml_df = pd.DataFrame(xml_list, columns = column_name)\r\n",
        "        return xml_df\r\n",
        "\r\n",
        "def create_tf_records(prm = BaseParameters()):\r\n",
        "    \"\"\"\r\n",
        "    TensorFlow record files creator\r\n",
        "    Keyword arguments:\r\n",
        "    prm     -- Parameters\r\n",
        "    \"\"\"\r\n",
        "    print(\"Creating TFRecord for the train images...\")\r\n",
        "    TFRecord().create_tf_record(\r\n",
        "        prm.train_images_dir,\r\n",
        "        os.path.join(prm.annotations_dir, 'train.record'),\r\n",
        "        os.path.join(prm.annotations_dir, 'label_map.pbtxt'))\r\n",
        "    print(\"Creating TFRecord for the evaluation images...\")\r\n",
        "    TFRecord().create_tf_record(\r\n",
        "        prm.eval_images_dir,\r\n",
        "        os.path.join(prm.annotations_dir, 'eval.record'))\r\n",
        "    shutil.copy2(os.path.join(prm.annotations_dir, 'label_map.pbtxt'), prm.model_dir)\r\n",
        "    print(f\"The labels map file was copied to {(os.path.join(str(Path(prm.model_dir).resolve()), 'label_map.pbtxt'))}\")\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    create_tf_records()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAfQxzEgTep"
      },
      "source": [
        "#Train pipeline configuration\r\n",
        "It configures the train pipeline using the original train pipeline of the pre-trained model but modifing some parameters as paths, number of labels, etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71d4Hl0lgRk2"
      },
      "source": [
        "import  os\r\n",
        "import  shutil\r\n",
        "\r\n",
        "def config_train_pipeline(prm = BaseParameters()):\r\n",
        "    \"\"\"\r\n",
        "    Configure the training pipeline\r\n",
        "    Keyword arguments:\r\n",
        "    prm     -- Parameters\r\n",
        "    \"\"\"\r\n",
        "    import tensorflow as tf\r\n",
        "    from object_detection.protos import pipeline_pb2\r\n",
        "    from google.protobuf import text_format\r\n",
        "    # Copy the pipeline configuration file if it's not already present in the output dir\r\n",
        "    print('Configuring the pipeline')\r\n",
        "    output_file = os.path.join(prm.annotations_dir, 'pipeline.config')\r\n",
        "    shutil.copy2(os.path.join(prm.pre_trained_model_dir, 'pipeline.config'), output_file)\r\n",
        "    # Configuring the pipeline\r\n",
        "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\r\n",
        "    with tf.io.gfile.GFile(output_file, \"r\") as f:\r\n",
        "        proto_str = f.read()\r\n",
        "        text_format.Merge(proto_str, pipeline_config)\r\n",
        "    pipeline_config.model.ssd.num_classes = 1 # TODO define\r\n",
        "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = prm.model['height']\r\n",
        "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = prm.model['width']\r\n",
        "    pipeline_config.train_config.batch_size = prm.model['batch_size']\r\n",
        "    pipeline_config.train_config.fine_tune_checkpoint = os.path.join(prm.pre_trained_model_dir, 'checkpoint', 'ckpt-0')\r\n",
        "    pipeline_config.train_config.fine_tune_checkpoint_type = 'detection'\r\n",
        "    pipeline_config.train_input_reader.label_map_path = os.path.join(prm.annotations_dir, 'label_map.pbtxt')\r\n",
        "    pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = os.path.join(prm.annotations_dir, 'train.record')\r\n",
        "    pipeline_config.eval_input_reader[0].label_map_path = os.path.join(prm.annotations_dir, 'label_map.pbtxt')\r\n",
        "    pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = os.path.join(prm.annotations_dir, 'eval.record')\r\n",
        "    config_text = text_format.MessageToString(pipeline_config)\r\n",
        "    with tf.io.gfile.GFile(output_file, 'wb') as f:\r\n",
        "        f.write(config_text)\r\n",
        "    shutil.copy2(output_file, prm.model_dir)\r\n",
        "    print('The train pipeline content is:')\r\n",
        "    print(str(config_text))\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    config_train_pipeline()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rqRbNl7iYRB"
      },
      "source": [
        "#Train loop\r\n",
        "The main train loop. It trains the model and put it in the output directory.\r\n",
        "\r\n",
        "It can be stopped before the complete train when a considerable result is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpMIIIUYiWkJ"
      },
      "source": [
        "import  os\r\n",
        "\r\n",
        "def main(_):\r\n",
        "    \"\"\"\r\n",
        "    Train loop function\r\n",
        "    \"\"\"\r\n",
        "    import tensorflow as tf\r\n",
        "    from object_detection import model_lib_v2\r\n",
        "    if prm.checkpoint_dir:\r\n",
        "        model_lib_v2.eval_continuously(\r\n",
        "            pipeline_config_path = os.path.join(prm.annotations_dir, 'pipeline.config'),\r\n",
        "            model_dir = prm.num_train_steps,\r\n",
        "            sample_1_of_n_eval_examples = prm.sample_1_of_n_eval_examples,\r\n",
        "            sample_1_of_n_eval_on_train_examples = prm.sample_1_of_n_eval_on_train_examples,\r\n",
        "            checkpoint_dir = prm.checkpoint_dir,\r\n",
        "            wait_interval = 300,\r\n",
        "            timeout = prm.eval_timeout)\r\n",
        "    else:\r\n",
        "        if prm.use_tpu:\r\n",
        "            # TPU is automatically inferred if tpu_name is None and\r\n",
        "            # we are running under cloud ai-platform.\r\n",
        "            resolver = tf.distribute.cluster_resolver.TPUClusterResolver(prm.tpu_name)\r\n",
        "            tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "            tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n",
        "        elif prm.num_workers > 1:\r\n",
        "            strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n",
        "        else:\r\n",
        "            strategy = tf.compat.v2.distribute.MirroredStrategy()\r\n",
        "    with strategy.scope():\r\n",
        "        print(prm.annotations_dir)\r\n",
        "        print(prm.model_dir)\r\n",
        "        print(prm.num_train_steps)\r\n",
        "        print(prm.use_tpu)\r\n",
        "        print(prm.checkpoint_every_n)\r\n",
        "        print(prm.record_summaries)\r\n",
        "        model_lib_v2.train_loop(\r\n",
        "            pipeline_config_path = os.path.join(prm.annotations_dir, 'pipeline.config'),\r\n",
        "            model_dir = prm.model_dir,\r\n",
        "            train_steps = prm.num_train_steps,\r\n",
        "            use_tpu = prm.use_tpu,\r\n",
        "            checkpoint_every_n = prm.checkpoint_every_n,\r\n",
        "            record_summaries = prm.record_summaries)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    import tensorflow as tf\r\n",
        "    tf.compat.v1.app.run()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}