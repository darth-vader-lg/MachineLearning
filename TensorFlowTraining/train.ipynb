{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oGKniD-fesc"
      },
      "source": [
        "---\r\n",
        "\r\n",
        "# Object detection train with TensorFlow 2.4.1\r\n",
        "This notebook will train a model for the object detection purpouse.\r\n",
        "It can be run as a Jupyter notebook in the Google Colab environment or exported as a Python file and run from a command line.\r\n",
        "\r\n",
        "This software detects automatically if you are working on a Colab environment or in your local machine.\r\n",
        "\r\n",
        "For a local machine it just requires a Python >= 3.7 installed.\r\n",
        "\r\n",
        "All the operations for installing all the required libraries and for preparing the data needed by the train algoritm will be done effortlessly for you.\r\n",
        "## Train preparation:\r\n",
        "*   Collect a set of images containing the objects that you want to train.\r\n",
        "*   Split the set in two different folders; one for the train and the other for the evaluation. The number of the evaluation images could be from 10% to 30% of the train images.\r\n",
        "*   Label the images using a standard images annotation tool as [labelImg](https://github.com/tzutalin/labelImg), [tVoTT](https://github.com/microsoft/VoTT), etc... and save the xml for each picture in the Coco format. \r\n",
        "*   Copy the folders with the prepared images set in your GDrive (if you are working on a Colab environment).\r\n",
        "*   Configure the train parameters listed in the next notebook's cell.\r\n",
        "\r\n",
        "## Train:\r\n",
        "Run the process and enjoy your time waiting for the train will be complete.\r\n",
        "\r\n",
        "You can also stop the train and restart again later; if you didn't clean the output directory for the model the train will restart from the last checkpoint, continuing the fine tuning of the model.\r\n",
        "The progress of the train can be followed by the Tensorboard (already included in this notebook).\r\n",
        "\r\n",
        "### For Colab environment train:\r\n",
        "The notebook needs to mount your GDrive. It will ask you the access authorization. Follow the instructions.\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9wR0KipJaPZ",
        "cellView": "form"
      },
      "source": [
        "# Module: default_cfg.py\n",
        "#@title #Notebook configuration\n",
        "#@markdown ## Data on Google Drive:\n",
        "#@markdown (The data will be treated in a Google Drive space if enabled)\n",
        "cfg_data_on_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown ## Base model:\n",
        "#@markdown (The base model from which the train will start)\n",
        "cfg_model_type = 'SSD MobileNet v2 320x320' #@param ['SSD MobileNet v2 320x320', 'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)']\n",
        "#@markdown ---\n",
        "#@markdown ## Images directories:\n",
        "#@markdown The GDrive directory (Colab execution) or the local directory (machine execution) where is located the images set for the train and for the evaluation.\n",
        "cfg_train_images_dir = 'images/train' #@param {type:\"string\"}\n",
        "cfg_eval_images_dir = 'images/eval' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## Train directory:\n",
        "#@markdown The GDrive directory (Colab execution) or the local directory (machine execution) where the checkpoints will be saved.\n",
        "cfg_trained_model = 'trained-model' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## Export directory:\n",
        "#@markdown The GDrive directory (Colab execution) or the local directory (machine execution) where the exported model will be saved.\n",
        "cfg_exported_model = 'exported-model' #@param {type:\"string\"}\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7g6Wr_PwP3O",
        "cellView": "form"
      },
      "source": [
        "# Module: mount_google_drive.py\r\n",
        "#@title #Mount Google Drive\r\n",
        "#@markdown Mounting of the Google Drive (if enabled in the configuration).\r\n",
        "\r\n",
        "import  os\r\n",
        "import  sys\r\n",
        "\r\n",
        "try:    from    default_cfg import *\r\n",
        "except: pass\r\n",
        "\r\n",
        "def mount_google_drive():\r\n",
        "    if (not os.path.exists('/mnt/MyDrive')):\r\n",
        "        print('Mounting the GDrive')\r\n",
        "        from google.colab import drive\r\n",
        "        drive.mount('/mnt')\r\n",
        "    else:\r\n",
        "        print('GDrive already mounted')\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    if (cfg_data_on_drive and 'google.colab' in sys.modules):\r\n",
        "        mount_google_drive()\r\n",
        "#@markdown ---\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV1W9gDHQcjf",
        "collapsed": true,
        "cellView": "form"
      },
      "source": [
        "# Module: model_types.py\n",
        "#@title #Model types { vertical-output: true, form-width: \"20%\" }\n",
        "#@markdown Initialize the list of the available pre-trained models and their parameters.\n",
        "\n",
        "\"\"\" List of the available models and their definitions \"\"\"\n",
        "models = {\n",
        "    'CenterNet Resnet101 V1 FPN 512x512': {\n",
        "        'dir_name': 'centernet_resnet101_v1_fpn_512x512_coco17_tpu-8',\n",
        "        'download_path': 'http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v1_fpn_512x512_coco17_tpu-8.tar.gz',\n",
        "        'batch_size': 8,\n",
        "        'height': 512,\n",
        "        'width': 512\n",
        "    },\n",
        "    'SSD MobileNet v2 320x320': {\n",
        "        'dir_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'download_path': 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "        'batch_size': 8,\n",
        "        'height': 300,\n",
        "        'width': 300\n",
        "    },\n",
        "    'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)': {\n",
        "        'dir_name': 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8',\n",
        "        'download_path': 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz',\n",
        "        'batch_size': 8,\n",
        "        'height': 640,\n",
        "        'width': 640\n",
        "    },\n",
        "}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import pprint\n",
        "    pprint.PrettyPrinter(1).pprint(models)\n",
        "    print('Dictionary of pre-trained models configured')\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zImaNt8dTqRj",
        "cellView": "form"
      },
      "source": [
        "# Module: utilities.py\n",
        "#@title #Utility functions\n",
        "#@markdown Some utility functions used for the train steps.\n",
        "\n",
        "import  os\n",
        "import  subprocess\n",
        "import  sys\n",
        "\n",
        "def execute_subprocess(cmd):\n",
        "    \"\"\"\n",
        "    Execute a subprocess returning each line of the standard output.\n",
        "    Keyword arguments:\n",
        "    cmd     -- the process to execute with its parameters\n",
        "    \"\"\"\n",
        "    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, universal_newlines=True)\n",
        "    for stdout_line in iter(popen.stdout.readline, \"\"):\n",
        "        yield stdout_line \n",
        "    popen.stdout.close()\n",
        "    return_code = popen.wait()\n",
        "    if return_code:\n",
        "        raise subprocess.CalledProcessError(return_code, cmd)\n",
        "\n",
        "def execute(cmd):\n",
        "    \"\"\"\n",
        "    Execute a subprocess printing its standard output.\n",
        "    Keyword arguments:\n",
        "    cmd     -- the process to execute with its parameters\n",
        "    \"\"\"\n",
        "    for output in execute_subprocess(cmd):\n",
        "        print(output, end=\"\")\n",
        "\n",
        "def execute_script(cmd):\n",
        "    \"\"\"\n",
        "    Execute a script as a subprocess printing its standard output.\n",
        "    Keyword arguments:\n",
        "    cmd     -- the parameters of the script\n",
        "    \"\"\"\n",
        "    python_path = os.path.join(os.path.dirname(sys.executable), 'python3')\n",
        "    if (not os.path.exists(python_path)):\n",
        "        python_path = os.path.join(os.path.dirname(sys.executable), 'python')\n",
        "    script_cmd = [python_path]\n",
        "    script_cmd.extend(cmd)\n",
        "    for output in execute_subprocess(script_cmd):\n",
        "        print(output, end=\"\")\n",
        "\n",
        "def get_type_of_script():\n",
        "    \"\"\"\n",
        "    Return of the type of the script is being executed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ipy_str = str(type(get_ipython()))\n",
        "        if ('ipykernel_launcher.py' in sys.argv[0]):\n",
        "            return 'jupyter'\n",
        "        return 'ipython'\n",
        "    except:\n",
        "        return 'terminal'\n",
        "\n",
        "def is_ipython():\n",
        "    \"\"\"\n",
        "    True if running in an ipython environment\n",
        "    \"\"\"\n",
        "    return get_type_of_script() == 'ipython'\n",
        "\n",
        "def is_jupyter():\n",
        "    \"\"\"\n",
        "    True if running in a jupyter notebook\n",
        "    \"\"\"\n",
        "    return get_type_of_script() == 'jupyter'\n",
        "\n",
        "def is_terminal():\n",
        "    \"\"\"\n",
        "    True if running a terminal environment\n",
        "    \"\"\"\n",
        "    return get_type_of_script() == 'terminal'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('Utilities functions initialized')\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE1vVxUaQycS",
        "cellView": "form"
      },
      "source": [
        "# Module: base_parameters.py\n",
        "#@title #Base parameters { form-width: \"20%\" }\n",
        "#@markdown Definition of the base parameters class.\n",
        "\n",
        "from    absl import flags\n",
        "import  os\n",
        "from    pathlib import Path\n",
        "import  sys\n",
        "import  tempfile\n",
        "\n",
        "try:    from    default_cfg import *\n",
        "except: pass\n",
        "try:    from    model_types import models\n",
        "except: pass\n",
        "\n",
        "class BaseParameters:\n",
        "    \"\"\" Class holding the base parameters \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" Constructor \"\"\"\n",
        "        self._model_type = cfg_model_type or 'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)'\n",
        "        self._model_dir = cfg_trained_model or 'trained-model'\n",
        "        self._train_images_dir = cfg_train_images_dir or 'images/train'\n",
        "        self._eval_images_dir = cfg_eval_images_dir or 'images/eval'\n",
        "        self._annotations_dir = 'annotations'\n",
        "        self._pre_trained_model_base_dir = os.path.join(tempfile.gettempdir(), 'tensorflow-pre-trained-models')\n",
        "        self._is_path = [\n",
        "            'model_dir',\n",
        "            'train_images_dir',\n",
        "            'eval_images_dir',\n",
        "            'annotations_dir']\n",
        "    default = None\n",
        "    @property\n",
        "    def model(self):\n",
        "        global models\n",
        "        return models[self.model_type]\n",
        "    @property\n",
        "    def model_type(self): return self._model_type\n",
        "    @model_type.setter\n",
        "    def model_type(self, value): self._model_type = value\n",
        "    @property\n",
        "    def model_dir(self): return self._model_dir\n",
        "    @model_dir.setter\n",
        "    def model_dir(self, value): self._model_dir = value\n",
        "    @property\n",
        "    def train_images_dir(self): return self._train_images_dir\n",
        "    @train_images_dir.setter\n",
        "    def train_images_dir(self, value): self._train_images_dir = value\n",
        "    @property\n",
        "    def eval_images_dir(self): return self._eval_images_dir\n",
        "    @eval_images_dir.setter\n",
        "    def eval_images_dir(self, value): self._eval_images_dir = value\n",
        "    @property\n",
        "    def annotations_dir(self): return self._annotations_dir\n",
        "    @annotations_dir.setter\n",
        "    def annotations_dir(self, value): self._annotations_dir = value\n",
        "    @property\n",
        "    def pre_trained_model_base_dir(self): return self._pre_trained_model_base_dir\n",
        "    @pre_trained_model_base_dir.setter\n",
        "    def pre_trained_model_base_dir(self, value): self._pre_trained_model_base_dir = value\n",
        "    def __str__(self):\n",
        "        result = ''\n",
        "        propnames = [p for p in dir(type(self)) if isinstance(getattr(type(self), p),property) and getattr(self, p)]\n",
        "        for prop in propnames:\n",
        "            try:\n",
        "                value = getattr(self, prop)\n",
        "                if (prop in self._is_path):\n",
        "                    value = str(Path(value).resolve())\n",
        "                if (len(result) > 0):\n",
        "                    result += '\\n'\n",
        "                result += f'{prop}: {value}'\n",
        "            except:\n",
        "                pass\n",
        "        return result\n",
        "    def update_flags(self):\n",
        "        propnames = [p for p in dir(type(self)) if isinstance(getattr(type(self), p),property) and getattr(self, p)]\n",
        "        for prop in propnames:\n",
        "            try:\n",
        "                value = getattr(self, prop)\n",
        "                if (value):\n",
        "                    setattr(flags.FLAGS, prop, value)\n",
        "                    print(f'Written flag {prop} with value {value}')\n",
        "            except:\n",
        "                pass\n",
        "    def update_values(self):\n",
        "        propnames = [p for p in dir(type(self)) if isinstance(getattr(type(self), p),property)]\n",
        "        for prop in propnames:\n",
        "            try:\n",
        "                value = getattr(flags.FLAGS, prop)\n",
        "                if (value):\n",
        "                    setattr(self, prop, value)\n",
        "                    print(f'Written property {prop} with value {value}')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "BaseParameters.default = BaseParameters.default or BaseParameters()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prm = ('prm' in locals() and isinstance(prm, BaseParameters) and prm) or BaseParameters.default\n",
        "    print(prm)\n",
        "    print('Base parameters configured')\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eECHryVmTLg",
        "cellView": "form"
      },
      "source": [
        "#module train_parameters.py\n",
        "#@title #Train parameters { form-width: \"20%\" }\n",
        "#@markdown Definition of the train parameters. Read the comments in the flags\n",
        "#@markdown section of the train main module\n",
        "#@markdown https://raw.githubusercontent.com/tensorflow/models/e356598a5b79a768942168b10d9c1acaa923bdb4/research/object_detection/model_main_tf2.py\n",
        "\n",
        "import  os\n",
        "\n",
        "try:    from    base_parameters import BaseParameters\n",
        "except: pass\n",
        "try:    from    default_cfg import *\n",
        "except: pass\n",
        "\n",
        "class TrainParameters(BaseParameters):\n",
        "    \"\"\" Class holding the train execution parameters \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" Constructor \"\"\"\n",
        "        super().__init__()\n",
        "        self._pipeline_config_path = os.path.join(self.annotations_dir, 'pipeline.config')\n",
        "        self._num_train_steps = None\n",
        "        self._eval_on_train_data = False\n",
        "        self._sample_1_of_n_eval_examples = None\n",
        "        self._sample_1_of_n_eval_on_train_examples = 5\n",
        "        self._checkpoint_dir = None\n",
        "        self._eval_timeout = 3600\n",
        "        self._use_tpu = False\n",
        "        self._tpu_name = None\n",
        "        self._num_workers = 1\n",
        "        self._checkpoint_every_n = 1000\n",
        "        self._record_summaries = True\n",
        "        self._is_path.extend([\n",
        "            'pipeline_config_path',\n",
        "            'checkpoint_dir'])\n",
        "    default = None\n",
        "    @property\n",
        "    def pipeline_config_path(self): return self._pipeline_config_path\n",
        "    @pipeline_config_path.setter\n",
        "    def pipeline_config_path(self, value): self._pipeline_config_path = value\n",
        "    @property\n",
        "    def num_train_steps(self): return self._num_train_steps\n",
        "    @num_train_steps.setter\n",
        "    def num_train_steps(self, value): self._num_train_steps = value\n",
        "    @property\n",
        "    def eval_on_train_data(self): return self._eval_on_train_data\n",
        "    @eval_on_train_data.setter\n",
        "    def eval_on_train_data(self, value): self._eval_on_train_data = value\n",
        "    @property\n",
        "    def sample_1_of_n_eval_examples(self): return self._sample_1_of_n_eval_examples\n",
        "    @sample_1_of_n_eval_examples.setter\n",
        "    def sample_1_of_n_eval_examples(self, value): self._sample_1_of_n_eval_examples = value\n",
        "    @property\n",
        "    def sample_1_of_n_eval_on_train_examples(self): return self._sample_1_of_n_eval_on_train_examples\n",
        "    @sample_1_of_n_eval_on_train_examples.setter\n",
        "    def sample_1_of_n_eval_on_train_examples(self, value): self._sample_1_of_n_eval_on_train_examples = value\n",
        "    @property\n",
        "    def checkpoint_dir(self): return self._checkpoint_dir\n",
        "    @checkpoint_dir.setter\n",
        "    def checkpoint_dir(self, value): self._checkpoint_dir = value\n",
        "    @property\n",
        "    def eval_timeout(self): return self._eval_timeout\n",
        "    @eval_timeout.setter\n",
        "    def eval_timeout(self, value): self._eval_timeout = value\n",
        "    @property\n",
        "    def use_tpu(self): return self._use_tpu\n",
        "    @use_tpu.setter\n",
        "    def use_tpu(self, value): self._use_tpu = value\n",
        "    @property\n",
        "    def tpu_name(self): return self._tpu_name\n",
        "    @tpu_name.setter\n",
        "    def tpu_name(self, value): self._tpu_name = value\n",
        "    @property\n",
        "    def num_workers(self): return self._num_workers\n",
        "    @num_workers.setter\n",
        "    def num_workers(self, value): self._num_workers = value\n",
        "    @property\n",
        "    def checkpoint_every_n(self): return self._checkpoint_every_n\n",
        "    @checkpoint_every_n.setter\n",
        "    def checkpoint_every_n(self, value): self._checkpoint_every_n = value\n",
        "    @property\n",
        "    def record_summaries(self): return self._record_summaries\n",
        "    @record_summaries.setter\n",
        "    def record_summaries(self, value): self._record_summaries = value\n",
        "\n",
        "TrainParameters.default = TrainParameters.default or TrainParameters()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prm = ('prm' in locals() and isinstance(prm, TrainParameters) and prm) or TrainParameters.default\n",
        "    print(prm)\n",
        "    print('Train parameters configured')\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czw15dMATgWa",
        "cellView": "form"
      },
      "source": [
        "# Module od_install.py\n",
        "#@title #Object detection libraries installation { form-width: \"20%\" }\n",
        "#@markdown This step installs a well known Python environment for the train.\n",
        "\n",
        "import  os\n",
        "import  datetime\n",
        "from    pathlib import Path\n",
        "import  shutil\n",
        "import  sys\n",
        "import  tempfile\n",
        "\n",
        "try:    from utilities import *\n",
        "except: pass\n",
        "\n",
        "def install_object_detection():\n",
        "    \"\"\"\n",
        "    Install a well known environment.\n",
        "    \"\"\"\n",
        "    # Upgrade pip and setuptools\n",
        "    is_installed = False\n",
        "    try:\n",
        "        import pip\n",
        "        is_installed = pip.__version__ == '21.0.1'\n",
        "    except: pass\n",
        "    if (not is_installed):\n",
        "        execute_script(['-m', 'pip', 'install', '--upgrade', 'pip==21.0.1'])\n",
        "    else:\n",
        "        print('pip 21.0.1 is already installed')\n",
        "    is_installed = False\n",
        "    try:\n",
        "        import setuptools\n",
        "        is_installed = setuptools.__version__ == '54.0.0'\n",
        "    except: pass\n",
        "    if (not is_installed):\n",
        "        execute_script(['-m', 'pip', 'install', '--upgrade', 'setuptools==54.0.0'])\n",
        "    else:\n",
        "        print('setuptools 54.0.0 is already installed')\n",
        "    # Install TensorFlow\n",
        "    is_installed = False\n",
        "    try:\n",
        "        import tensorflow\n",
        "        is_installed = tensorflow.__version__ == '2.4.1'\n",
        "    except: pass\n",
        "    if (not is_installed):\n",
        "        execute_script(['-m', 'pip', 'install', 'tensorflow==2.4.1'])\n",
        "    else:\n",
        "        print('TensorFlow 2.4.1 is already installed')\n",
        "    # Install pygit2\n",
        "    is_installed = False\n",
        "    try:\n",
        "        import pygit2\n",
        "        is_installed = pygit2.__version__ == '1.5.0'\n",
        "    except: pass\n",
        "    if (not is_installed):\n",
        "        execute_script(['-m', 'pip', 'install', 'pygit2==1.5.0'])\n",
        "        import pygit2\n",
        "    else:\n",
        "        print('pygit2 1.5.0 is already installed')\n",
        "    # Directory of the TensorFlow object detection api and commit id\n",
        "    od_api_dir = os.path.join(tempfile.gettempdir(), 'tensorflow-object-detection-api-2.4.1')\n",
        "    od_api_ish = 'e356598a5b79a768942168b10d9c1acaa923bdb4'\n",
        "    # Install the object detection api\n",
        "    is_installed = False\n",
        "    try:\n",
        "        import object_detection\n",
        "        repo = pygit2.Repository(od_api_dir)\n",
        "        if (repo.head.target.hex == od_api_ish):\n",
        "            is_installed = True\n",
        "    except: pass\n",
        "    # Install the TensorFlow models\n",
        "    if (not is_installed):\n",
        "        # Progress class for the git output\n",
        "        class GitCallbacks(pygit2.RemoteCallbacks):\n",
        "            def __init__(self, credentials=None, certificate=None):\n",
        "                self.dateTime = datetime.datetime.now()\n",
        "                return super().__init__(credentials=credentials, certificate=certificate)\n",
        "            def transfer_progress(self, stats):\n",
        "                now = datetime.datetime.now()\n",
        "                if ((now - self.dateTime).total_seconds() > 1):\n",
        "                    print('\\rReceiving... Deltas [%d / %d], Objects [%d / %d]'%(stats.indexed_deltas, stats.total_deltas, stats.indexed_objects, stats.total_objects), end='', flush=True)\n",
        "                    self.dateTime = now\n",
        "                if (stats.received_objects >= stats.total_objects and stats.indexed_objects >= stats.total_objects and stats.indexed_deltas >= stats.total_deltas):\n",
        "                    print('\\r\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\rDone Deltas %d, Objects %d.'%(stats.total_objects, stats.total_objects))\n",
        "                return super().transfer_progress(stats)\n",
        "        # Create the callback for the progress\n",
        "        callbacks = GitCallbacks();\n",
        "        # Clone the TensorFlow models repository\n",
        "        print('Cloning the TensorFlow object detection api repository')\n",
        "        pygit2.clone_repository('https://github.com/tensorflow/models.git', od_api_dir, callbacks = callbacks)\n",
        "        print('TensorFlow object detection api repository cloned')\n",
        "        # Checkout the well known commit\n",
        "        repo = pygit2.Repository(od_api_dir)\n",
        "        (commit, reference) = repo.resolve_refish(od_api_ish)\n",
        "        repo.checkout_tree(commit)\n",
        "        repo.reset(pygit2.Oid(hex=od_api_ish), pygit2.GIT_RESET_HARD)\n",
        "        # Move to the research dir\n",
        "        currentDir = os.getcwd()\n",
        "        os.chdir(os.path.join(od_api_dir, 'research'))\n",
        "        # Install the protobuf tools\n",
        "        execute_script(['-m', 'pip', 'install', 'grpcio-tools==1.32.0'])\n",
        "        # Compile the protobufs\n",
        "        import grpc_tools.protoc as protoc\n",
        "        protoFiles = Path('object_detection/protos').rglob('*.proto')\n",
        "        for protoFile in protoFiles:\n",
        "            protoFilePath = str(protoFile)\n",
        "            print('Compiling', protoFilePath)\n",
        "            protoc.main(['grpc_tools.protoc', '--python_out=.', protoFilePath])\n",
        "        # Install the object detection packages\n",
        "        shutil.copy2('object_detection/packages/tf2/setup.py', '.')\n",
        "        execute_script(['-m', 'pip', 'install', '.'])\n",
        "        os.chdir(currentDir)\n",
        "    else:\n",
        "        print(f'TensorFlow object detection api SHA-1 {od_api_ish} is already installed')\n",
        "    sys.path.append(os.path.join(od_api_dir, 'research'))\n",
        "    sys.path.append(os.path.join(od_api_dir, 'research/slim'))\n",
        "    sys.path.append(os.path.join(od_api_dir, 'research/object_detection'))\n",
        "    print('Installation ok.')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    install_object_detection()\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JzO6ORpVBkP",
        "cellView": "form"
      },
      "source": [
        "# Module train_environment.py\n",
        "#@title #Train's environment initialization { form-width: \"30%\" }\n",
        "#@markdown In this section the environment for the training will be initialized.\n",
        "#@markdown\n",
        "#@markdown All necessary directories will be crated and the Google drive\n",
        "#@markdown containing the images will be mounted. Follow the instruction for the mounting during the execution.\n",
        "\n",
        "import  os\n",
        "from    pathlib import Path\n",
        "import  sys\n",
        "\n",
        "try:    from    default_cfg import *\n",
        "except: pass\n",
        "try:    from    train_parameters import TrainParameters\n",
        "except: pass\n",
        "\n",
        "def init_train_environment(prm: TrainParameters):\n",
        "    \"\"\"\n",
        "    Initialize the training environment with the right directories structure.\n",
        "    Keyword arguments:\n",
        "    prm     -- the train parameters\n",
        "    \"\"\"\n",
        "    # Set the configuration for Google Colab\n",
        "    if ('google.colab' in sys.modules and cfg_data_on_drive):\n",
        "        if (not os.path.exists('/mnt/MyDrive')):\n",
        "            print('Mounting the GDrive')\n",
        "            from google.colab import drive\n",
        "            drive.mount('/mnt')\n",
        "        # Check the existence of the train images dir\n",
        "        gdrive_dir = os.path.join('/mnt', 'MyDrive', prm.train_images_dir)\n",
        "        if (not os.path.isdir(gdrive_dir)):\n",
        "            raise Exception('Error!!! The train images dir doesn`t exist')\n",
        "        if (os.path.exists('/content/train-images')):\n",
        "            os.unlink('/content/train-images')\n",
        "        os.symlink(gdrive_dir, '/content/train-images', True)\n",
        "        print(f\"Google drive's {prm.train_images_dir} is linked to /content/train-images\")\n",
        "        prm.train_images_dir = '/content/train-images'\n",
        "        # Check the existence of the evaluation images dir\n",
        "        gdrive_dir = os.path.join('/mnt', 'MyDrive', prm.eval_images_dir)\n",
        "        if (not os.path.isdir(gdrive_dir)):\n",
        "            raise Exception('Error!!! The evaluation images dir doesn`t exist')\n",
        "        if (os.path.exists('/content/eval-images')):\n",
        "            os.unlink('/content/eval-images')\n",
        "        os.symlink(gdrive_dir, '/content/eval-images', True)\n",
        "        print(f\"Google drive's {prm.eval_images_dir} is linked to /content/eval-images\")\n",
        "        prm.eval_images_dir = '/content/eval-images'\n",
        "        # Check the existence of the output directory\n",
        "        gdrive_dir = os.path.join('/mnt', 'MyDrive', prm.model_dir)\n",
        "        if (not os.path.isdir(gdrive_dir)):\n",
        "            print('Creating the output directory')\n",
        "            os.mkdir(gdrive_dir)\n",
        "        if (os.path.exists('/content/trained-model')):\n",
        "            os.unlink('/content/trained-model')\n",
        "        os.symlink(gdrive_dir, '/content/trained-model', True)\n",
        "        print(f\"Google drive's {prm.model_dir} is linked to /content/trained-model\")\n",
        "        prm.model_dir = '/content/trained-model'\n",
        "    else:\n",
        "        if (not os.path.isdir(prm.train_images_dir)):\n",
        "            raise Exception('Error!!! The train images dir doesn`t exist')\n",
        "        print(f'Train images from {str(Path(prm.train_images_dir).resolve())}')\n",
        "        if (not os.path.isdir(prm.eval_images_dir)):\n",
        "            raise Exception('Error!!! The evaluation images dir doesn`t exist')\n",
        "        print(f'Train images from {str(Path(prm.eval_images_dir).resolve())}')\n",
        "        if (not os.path.exists(prm.model_dir)):\n",
        "            print('Creating the output directory')\n",
        "            os.mkdir(prm.model_dir)\n",
        "        print(f'The trained model will be in {str(Path(prm.model_dir).resolve())}')\n",
        "    if (not os.path.exists(prm.annotations_dir)):\n",
        "        os.mkdir(prm.annotations_dir)\n",
        "    print(f'The annotations files will be in {str(Path(prm.annotations_dir).resolve())}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prm = ('prm' in locals() and isinstance(prm, TrainParameters) and prm) or TrainParameters.default\n",
        "    init_train_environment(prm)\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItFyPSm6dfgL",
        "cellView": "form"
      },
      "source": [
        "# Module: pretrained_model.py\n",
        "#@title #Pre-trained model download { form-width: \"20%\" }\n",
        "#@markdown Download of the pre-trained model from the TensorFlow 2 model zoo.\n",
        "\n",
        "import  os\n",
        "from    pathlib import Path\n",
        "from    urllib import request\n",
        "\n",
        "try:\n",
        "    from    base_parameters import BaseParameters\n",
        "except: pass\n",
        "\n",
        "def download_pretrained_model(prm: BaseParameters):\n",
        "    \"\"\"\n",
        "    Download from the TensorFlow model zoo the pre-trained model.\n",
        "    Keyword arguments:\n",
        "    prm     -- the base parameters\n",
        "    \"\"\"\n",
        "    pre_trained_model_dir = os.path.join(prm.pre_trained_model_base_dir, prm.model['dir_name'])\n",
        "    if (not os.path.exists(pre_trained_model_dir) or not os.path.exists(os.path.join(pre_trained_model_dir, 'checkpoint', 'checkpoint'))):\n",
        "        if (not os.path.exists(prm.pre_trained_model_base_dir)):\n",
        "            os.mkdir(prm.pre_trained_model_base_dir)\n",
        "        pre_trained_model_file = pre_trained_model_dir + '.tar.gz'\n",
        "        print(f'Downloading the pre-trained model {str(Path(pre_trained_model_file).name)}...')\n",
        "        import tarfile\n",
        "        request.urlretrieve(prm.model['download_path'], pre_trained_model_file) # TODO: show progress\n",
        "        print('Done.')\n",
        "        print(f'Extracting the pre-trained model {str(Path(pre_trained_model_file).name)}...')\n",
        "        tar = tarfile.open(pre_trained_model_file)\n",
        "        tar.extractall(prm.pre_trained_model_base_dir)\n",
        "        tar.close()\n",
        "        os.remove(pre_trained_model_file)\n",
        "    print(f'Pre-trained model is located at {str(Path(pre_trained_model_dir).resolve())}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prm = ('prm' in locals() and isinstance(prm, BaseParameters) and prm) or BaseParameters.default\n",
        "    download_pretrained_model(prm)\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAqirnmAfq1P",
        "cellView": "form"
      },
      "source": [
        "# Module: tf_records.py\n",
        "#@title #TensorFlow's records { form-width: \"30%\" }\n",
        "#@markdown In this step there will be created the TensorFlow records from the\n",
        "#@markdown annotated images and the file contained all the labels' indices.\n",
        "\n",
        "import  glob\n",
        "import  io\n",
        "import  os\n",
        "from    pathlib import Path\n",
        "import  shutil\n",
        "\n",
        "try:\n",
        "    from    base_parameters import BaseParameters\n",
        "except: pass\n",
        "\n",
        "class TFRecord:\n",
        "    \"\"\"Class for the TensorFlow records creation\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" Constructor \"\"\"\n",
        "        super().__init__()\n",
        "        self._label_set = set()\n",
        "        self._label_dict = dict()\n",
        "    def class_text_to_int(self, row_label):\n",
        "        \"\"\"\n",
        "        Convertion of the text of the labels to an integer index\n",
        "        Keyword arguments:\n",
        "        row_label   -- the label to convert to int\n",
        "        \"\"\"\n",
        "        if (len(self._label_dict) == 0):\n",
        "            count = len(self._label_set)\n",
        "            labelIx = 1\n",
        "            for label in self._label_set:\n",
        "                self._label_dict[label] = labelIx\n",
        "                labelIx += 1\n",
        "        return self._label_dict[row_label]\n",
        "    def create_tf_example(self, group, path):\n",
        "        \"\"\"\n",
        "        TensorFlow example creator\n",
        "        Keyword arguments:\n",
        "        group   -- group's name\n",
        "        path    -- path of the labeled images\n",
        "        \"\"\"\n",
        "        from object_detection.utils import dataset_util\n",
        "        from PIL import Image\n",
        "        import tensorflow as tf\n",
        "        with tf.compat.v1.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "            encoded_jpg = fid.read()\n",
        "        encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "        image = Image.open(encoded_jpg_io)\n",
        "        width, height = image.size\n",
        "        filename = group.filename.encode('utf8')\n",
        "        image_format = b'jpg'\n",
        "        xmins = []\n",
        "        xmaxs = []\n",
        "        ymins = []\n",
        "        ymaxs = []\n",
        "        classes_text = []\n",
        "        classes = []\n",
        "        for index, row in group.object.iterrows():\n",
        "            xmins.append(row['xmin'] / width)\n",
        "            xmaxs.append(row['xmax'] / width)\n",
        "            ymins.append(row['ymin'] / height)\n",
        "            ymaxs.append(row['ymax'] / height)\n",
        "            classes_text.append(row['class'].encode('utf8'))\n",
        "            classes.append(self.class_text_to_int(row['class']))\n",
        "        tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "            'image/height': dataset_util.int64_feature(height),\n",
        "            'image/width': dataset_util.int64_feature(width),\n",
        "            'image/filename': dataset_util.bytes_feature(filename),\n",
        "            'image/source_id': dataset_util.bytes_feature(filename),\n",
        "            'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "            'image/format': dataset_util.bytes_feature(image_format),\n",
        "            'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "            'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "            'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "            'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "            'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "            'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "        }))\n",
        "        return tf_example\n",
        "    def create_tf_record(self, image_dir, output_file, labels_file = None, csv_file = None):\n",
        "        \"\"\"\n",
        "        TensorFlow record creator\n",
        "        Keyword arguments:\n",
        "        image_dir   -- the directory containing the images\n",
        "        output_file -- the output file path and name\n",
        "        labels_file -- the optional output file path and name of the resulting labels file\n",
        "        csv_file    -- the optional output file path and name of the csv file\n",
        "        \"\"\"\n",
        "        import tensorflow as tf\n",
        "        writer = tf.compat.v1.python_io.TFRecordWriter(output_file)\n",
        "        path = os.path.join(image_dir)\n",
        "        examples = self.xml_to_csv(image_dir)\n",
        "        grouped = self.split(examples, 'filename')\n",
        "        for group in grouped:\n",
        "            tf_example = self.create_tf_example(group, path)\n",
        "            writer.write(tf_example.SerializeToString())\n",
        "        writer.close()\n",
        "        print(f'Created the TFRecord file {str(Path(output_file).resolve())}')\n",
        "        if labels_file is not None:\n",
        "            from google.protobuf import text_format\n",
        "            from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n",
        "            msg = StringIntLabelMap()\n",
        "            for id, name in enumerate(self._label_set, start = 1):\n",
        "                msg.item.append(StringIntLabelMapItem(id = id, name = name))\n",
        "            text = str(text_format.MessageToBytes(msg, as_utf8 = True), 'utf-8')\n",
        "            with open(labels_file, 'w') as f:\n",
        "                f.write(text)\n",
        "            print(f'Created the labels map file {str(Path(labels_file).resolve())}')\n",
        "        if csv_file is not None:\n",
        "            examples.to_csv(csv_file, index = None)\n",
        "            print(f'Created the CSV file {str(Path(csv_file).resolve())}')\n",
        "    def split(self, df, group):\n",
        "        \"\"\"\n",
        "        Split the labels in an image\n",
        "        Keyword arguments:\n",
        "        df      -- TensorFlow example\n",
        "        group   -- group's name\n",
        "        \"\"\"\n",
        "        from collections import namedtuple\n",
        "        data = namedtuple('data', ['filename', 'object'])\n",
        "        gb = df.groupby(group)\n",
        "        return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "    def xml_to_csv(self, path):\n",
        "        \"\"\"\n",
        "        Convert the xml files generated by labeling image softwares into the cvs panda format\n",
        "        Keyword arguments:\n",
        "        path    -- Path of the generated csv file\n",
        "        \"\"\"\n",
        "        import pandas as pd\n",
        "        import xml.etree.ElementTree as ET\n",
        "        xml_list = []\n",
        "        for xml_file in glob.glob(path + '/*.xml'):\n",
        "            tree = ET.parse(xml_file)\n",
        "            root = tree.getroot()\n",
        "            for member in root.findall('object'):\n",
        "                value = (\n",
        "                    root.find('filename').text,\n",
        "                    int(root.find('size')[0].text),\n",
        "                    int(root.find('size')[1].text),\n",
        "                    member[0].text,\n",
        "                    int(member[4][0].text),\n",
        "                    int(member[4][1].text),\n",
        "                    int(member[4][2].text),\n",
        "                    int(member[4][3].text))\n",
        "                xml_list.append(value)\n",
        "                self._label_set.add(member[0].text)\n",
        "        column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "        xml_df = pd.DataFrame(xml_list, columns = column_name)\n",
        "        return xml_df\n",
        "\n",
        "def create_tf_records(prm: BaseParameters):\n",
        "    \"\"\"\n",
        "    TensorFlow record files creator\n",
        "    Keyword arguments:\n",
        "    prm     -- Parameters\n",
        "    \"\"\"\n",
        "    print(\"Creating TFRecord for the train images...\")\n",
        "    TFRecord().create_tf_record(\n",
        "        prm.train_images_dir,\n",
        "        os.path.join(prm.annotations_dir, 'train.record'),\n",
        "        os.path.join(prm.annotations_dir, 'label_map.pbtxt'))\n",
        "    print(\"Creating TFRecord for the evaluation images...\")\n",
        "    TFRecord().create_tf_record(\n",
        "        prm.eval_images_dir,\n",
        "        os.path.join(prm.annotations_dir, 'eval.record'))\n",
        "    shutil.copy2(os.path.join(prm.annotations_dir, 'label_map.pbtxt'), prm.model_dir)\n",
        "    print(f\"The labels map file was copied to {(os.path.join(str(Path(prm.model_dir).resolve()), 'label_map.pbtxt'))}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prm = ('prm' in locals() and isinstance(prm, BaseParameters) and prm) or BaseParameters.default\n",
        "    create_tf_records(prm)\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71d4Hl0lgRk2",
        "cellView": "form"
      },
      "source": [
        "# Module: train_pipeline\n",
        "#@title #Train pipeline configuration { form-width: \"20%\" }\n",
        "#@markdown Configuration of the train pipeline using the original train pipeline\n",
        "#@markdown of the pre-trained model but modifing some parameters as paths,\n",
        "#@markdown number of labels, etc... \n",
        "\n",
        "import  os\n",
        "import  shutil\n",
        "\n",
        "try:\n",
        "    from    train_parameters import TrainParameters\n",
        "except: pass\n",
        "\n",
        "def config_train_pipeline(prm: TrainParameters):\n",
        "    \"\"\"\n",
        "    Configure the training pipeline\n",
        "    Keyword arguments:\n",
        "    prm     -- Parameters\n",
        "    \"\"\"\n",
        "    import  tensorflow as tf\n",
        "    from    object_detection.protos import pipeline_pb2\n",
        "    from    object_detection.utils import label_map_util\n",
        "    from    google.protobuf import text_format\n",
        "    import  tempfile\n",
        "    # Copy the pipeline configuration file if it's not already present in the output dir\n",
        "    print('Configuring the pipeline')\n",
        "    output_file = prm.pipeline_config_path\n",
        "    pre_trained_model_dir = os.path.join(prm.pre_trained_model_base_dir, prm.model['dir_name'])\n",
        "    pre_trained_cfg_file = os.path.join(\n",
        "        tempfile.gettempdir(),\n",
        "        'tensorflow-object-detection-api-2.4.1',\n",
        "        'research', 'object_detection', 'configs', 'tf2',\n",
        "        prm.model['dir_name'] + '.config')\n",
        "    shutil.copy2(pre_trained_cfg_file, output_file)\n",
        "    # Read the number of labels\n",
        "    label_dict = label_map_util.get_label_map_dict(os.path.join(prm.annotations_dir, 'label_map.pbtxt'))\n",
        "    labels_count = len(label_dict)\n",
        "    # Configuring the pipeline\n",
        "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "    with tf.io.gfile.GFile(output_file, \"r\") as f:\n",
        "        proto_str = f.read()\n",
        "        text_format.Merge(proto_str, pipeline_config)\n",
        "    pipeline_config.model.ssd.num_classes = labels_count\n",
        "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = prm.model['height']\n",
        "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = prm.model['width']\n",
        "    pipeline_config.train_config.batch_size = prm.model['batch_size']\n",
        "    pipeline_config.train_config.fine_tune_checkpoint = os.path.join(pre_trained_model_dir, 'checkpoint', 'ckpt-0')\n",
        "    pipeline_config.train_config.fine_tune_checkpoint_type = 'detection'\n",
        "    pipeline_config.train_input_reader.label_map_path = os.path.join(prm.annotations_dir, 'label_map.pbtxt')\n",
        "    pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = os.path.join(prm.annotations_dir, 'train.record')\n",
        "    pipeline_config.eval_input_reader[0].label_map_path = os.path.join(prm.annotations_dir, 'label_map.pbtxt')\n",
        "    pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = os.path.join(prm.annotations_dir, 'eval.record')\n",
        "    config_text = text_format.MessageToString(pipeline_config)\n",
        "    with tf.io.gfile.GFile(output_file, 'wb') as f:\n",
        "        f.write(config_text)\n",
        "    shutil.copy2(output_file, prm.model_dir)\n",
        "    print('The train pipeline content is:')\n",
        "    print(str(config_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prm = ('prm' in locals() and isinstance(prm, TrainParameters) and prm) or TrainParameters.default\n",
        "    config_train_pipeline(prm)\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9wWk4VwDnL4",
        "cellView": "form"
      },
      "source": [
        "# Module: train_tensorboard\n",
        "#@title #Start the TensorBoard { vertical-output: true }\n",
        "#@markdown The TensorBoard is run for checking the progress.\n",
        "#@markdown\n",
        "#@markdown Warning: an error message will be displayed if no data are yet present.\n",
        "#@markdown Wait that the train it will be started (loss messages on output) and\n",
        "#@markdown just click the refresh button.\n",
        "\n",
        "try:    from base_parameters import BaseParameters\n",
        "except: pass\n",
        "import  subprocess\n",
        "import  time\n",
        "try:    from utilities import *\n",
        "except: pass\n",
        "\n",
        "def start_tensorboard(prm: BaseParameters):\n",
        "    log_dir = os.path.join(prm.model_dir, 'train')\n",
        "    error = False\n",
        "    try:\n",
        "        subprocess.Popen(\n",
        "            ['tensorboard', '--logdir', log_dir],\n",
        "            stdout = subprocess.PIPE,\n",
        "            universal_newlines = True)\n",
        "    except:\n",
        "        try:\n",
        "            tensorboard_path = os.path.join(os.path.dirname(sys.executable), 'tensorboard')\n",
        "            subprocess.Popen(\n",
        "                [tensorboard_path, '--logdir', log_dir],\n",
        "                stdout = subprocess.PIPE,\n",
        "                universal_newlines = True)\n",
        "        except:\n",
        "            print('Warning: cannot start tensorboard')\n",
        "            error = True\n",
        "    if (not error and is_jupyter()):\n",
        "        import tensorboard\n",
        "        for i in range(5):\n",
        "            try:\n",
        "                tensorboard.notebook.display()\n",
        "                break\n",
        "            except:\n",
        "                time.sleep(1)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prm = ('prm' in locals() and isinstance(prm, BaseParameters) and prm) or BaseParameters.default\n",
        "    start_tensorboard(prm)\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug1afNGQ-J9b",
        "cellView": "form"
      },
      "source": [
        "# Module: train_main.py\n",
        "#@title #Train { form-width: \"20%\" }\n",
        "#@markdown The main train loop. It trains the model and put it in the output directory.\n",
        "#@markdown \n",
        "#@markdown It can be stopped before the completion when\n",
        "#@markdown a considerable result is reached and restart after for enhancing the tuning.\n",
        "\n",
        "from    absl import flags\n",
        "import  os\n",
        "import  sys\n",
        "\n",
        "try:    from    utilities import *\n",
        "except: pass\n",
        "\n",
        "# Avoiding the absl error for duplicated flags if run again the cell from a notebook\n",
        "for f in flags.FLAGS.flag_values_dict():\n",
        "    flags.FLAGS[f].allow_override = True\n",
        "\n",
        "# Flags for arguments parameters\n",
        "flags.DEFINE_string('model_type', None, 'Type of the base model.')\n",
        "flags.DEFINE_string('train_images_dir', None, 'Path to the directory '\n",
        "                    'containing the images for train and their labeling xml.')\n",
        "flags.DEFINE_string('eval_images_dir', None, 'Path to the directory '\n",
        "                    'containing the images for evaluate and their labeling xml.')\n",
        "\n",
        "def train_main(unused_argv):\n",
        "    # Part of code not executed on Colab notebook\n",
        "    def run_py_mode():\n",
        "        # Init the train environment\n",
        "        from pretrained_model import download_pretrained_model\n",
        "        from tf_records import create_tf_records\n",
        "        from train_environment import init_train_environment\n",
        "        from train_parameters import TrainParameters\n",
        "        from train_pipeline import config_train_pipeline\n",
        "        train_parameters = TrainParameters()\n",
        "        train_parameters.update_values()\n",
        "        init_train_environment(train_parameters)\n",
        "        download_pretrained_model(train_parameters)\n",
        "        create_tf_records(train_parameters)\n",
        "        config_train_pipeline(train_parameters)\n",
        "        # Import the train main function\n",
        "        from object_detection import model_main_tf2\n",
        "        train_parameters.update_flags()\n",
        "        # Start the tensorboard\n",
        "        from train_tensorboard import start_tensorboard\n",
        "        start_tensorboard(train_parameters)\n",
        "        # Execute the train\n",
        "        model_main_tf2.main(unused_argv)\n",
        "    def run_notebook_mode():\n",
        "        # Import the train main function\n",
        "        from object_detection import model_main_tf2\n",
        "        prm.update_flags()\n",
        "        # Execute the train\n",
        "        model_main_tf2.main(unused_argv)\n",
        "    # Execution\n",
        "    if (is_jupyter()):\n",
        "        run_notebook_mode()\n",
        "    else:\n",
        "        run_py_mode()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if (not is_jupyter()):\n",
        "        from od_install import install_object_detection\n",
        "        install_object_detection()\n",
        "    import tensorflow as tf\n",
        "    try:\n",
        "        tf.compat.v1.app.run(train_main)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Train interrupted by user')\n",
        "        pass\n",
        "    except SystemExit:\n",
        "        print('Train complete')\n",
        "        pass\n",
        "    else:\n",
        "        print('Train complete')\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTfmKCxytpJG",
        "cellView": "form"
      },
      "source": [
        "#module export_parameters.py\r\n",
        "#@title #Export parameters { form-width: \"20%\" }\r\n",
        "#@markdown Definition of the export parameters. Read the comments in the flags\r\n",
        "#@markdown section of the exporter main module\r\n",
        "#@markdown https://raw.githubusercontent.com/tensorflow/models/e356598a5b79a768942168b10d9c1acaa923bdb4/research/object_detection/exporter_main_v2.py\r\n",
        "\r\n",
        "import  os\r\n",
        "\r\n",
        "try:    from    base_parameters import BaseParameters\r\n",
        "except: pass\r\n",
        "try:    from    default_cfg import *\r\n",
        "except: pass\r\n",
        "\r\n",
        "class ExportParameters(BaseParameters):\r\n",
        "    \"\"\" Class holding the model export parameters \"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        \"\"\" Constructor \"\"\"\r\n",
        "        super().__init__()\r\n",
        "        self._input_type = 'image_tensor'\r\n",
        "        self._pipeline_config_path = os.path.join(self.model_dir, 'pipeline.config')\r\n",
        "        self._trained_checkpoint_dir = self.model_dir\r\n",
        "        self._output_directory = cfg_exported_model or 'exported-model'\r\n",
        "        self._is_path.extend([\r\n",
        "            'pipeline_config_path',\r\n",
        "            'trained_checkpoint_dir',\r\n",
        "            'output_directory'])\r\n",
        "    default = None\r\n",
        "    @property\r\n",
        "    def pipeline_config_path(self): return self._pipeline_config_path\r\n",
        "    @pipeline_config_path.setter\r\n",
        "    def pipeline_config_path(self, value): self._pipeline_config_path = value\r\n",
        "    @property\r\n",
        "    def trained_checkpoint_dir(self): return self._trained_checkpoint_dir\r\n",
        "    @trained_checkpoint_dir.setter\r\n",
        "    def trained_checkpoint_dir(self, value): self._trained_checkpoint_dir = value\r\n",
        "    @property\r\n",
        "    def output_directory(self): return self._output_directory\r\n",
        "    @output_directory.setter\r\n",
        "    def output_directory(self, value): self._output_directory = value\r\n",
        "\r\n",
        "ExportParameters.default = ExportParameters.default or ExportParameters()\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    prm = ('prm' in locals() and isinstance(prm, ExportParameters) and prm) or ExportParameters.default\r\n",
        "    print(prm)\r\n",
        "    print('Export parameters configured')\r\n",
        "\r\n",
        "#@markdown ---\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtousFSlt07O",
        "cellView": "form"
      },
      "source": [
        "# Module export_environment.py\r\n",
        "#@title #Export's environment initialization { form-width: \"30%\" }\r\n",
        "#@markdown In this section the environment for the export will be initialized.\r\n",
        "#@markdown\r\n",
        "#@markdown All necessary directories will be mounted from the Google drive.\r\n",
        "#@markdown Follow the instruction for the mounting during the execution.\r\n",
        "\r\n",
        "import  os\r\n",
        "from    pathlib import Path\r\n",
        "import  shutil\r\n",
        "import  sys\r\n",
        "\r\n",
        "try:    from    default_cfg import *\r\n",
        "except: pass\r\n",
        "try:    from    export_parameters import ExportParameters\r\n",
        "except: pass\r\n",
        "\r\n",
        "def init_export_environment(prm: ExportParameters):\r\n",
        "    \"\"\"\r\n",
        "    Initialize the model export environment with the right directories structure.\r\n",
        "    Keyword arguments:\r\n",
        "    prm     -- the export parameters\r\n",
        "    \"\"\"\r\n",
        "    # Set the configuration for Google Colab\r\n",
        "    if ('google.colab' in sys.modules and cfg_data_on_drive):\r\n",
        "        if (not os.path.exists('/mnt/MyDrive')):\r\n",
        "            print('Mounting the GDrive')\r\n",
        "            from google.colab import drive\r\n",
        "            drive.mount('/mnt')\r\n",
        "\r\n",
        "        # Check the existence of the checkpoints directory\r\n",
        "        gdrive_dir = os.path.join('/mnt', 'MyDrive', prm.trained_checkpoint_dir)\r\n",
        "        if (not os.path.isdir(gdrive_dir)):\r\n",
        "            raise Exception('Error!!! The trained checkpoint dir doesn`t exist')\r\n",
        "        if (os.path.exists('/content/trained-model')):\r\n",
        "            os.unlink('/content/trained-model')\r\n",
        "        os.symlink(gdrive_dir, '/content/trained-model', True)\r\n",
        "        print(f\"Google drive's {prm.trained_checkpoint_dir} is linked to /content/trained-model\")\r\n",
        "        prm.trained_checkpoint_dir = '/content/trained-model'\r\n",
        "        # Check the existence of the output directory\r\n",
        "        gdrive_dir = os.path.join('/mnt', 'MyDrive', prm.output_directory)\r\n",
        "        if (not os.path.isdir(gdrive_dir)):\r\n",
        "            print('Creating the output directory')\r\n",
        "            os.mkdir(gdrive_dir)\r\n",
        "        if (str(Path(prm.output_directory).resolve()) == str(Path(prm.model_dir).resolve())):\r\n",
        "            raise Exception(\"Error: export directory cannot be the train directory\")\r\n",
        "        if (os.path.exists('/content/exported-model')):\r\n",
        "            os.unlink('/content/exported-model')\r\n",
        "        os.symlink(gdrive_dir, '/content/exported-model', True)\r\n",
        "        gdrive_dir = os.path.join(prm.output_directory, 'exported-model')\r\n",
        "        print(f\"Google drive's {gdrive_dir} is linked to /content/exported-model\")\r\n",
        "        prm.output_directory = '/content/exported-model'\r\n",
        "    else:\r\n",
        "        if (not os.path.isdir(prm.trained_checkpoint_dir)):\r\n",
        "            raise Exception('Error!!! The trained checkpoint dir doesn`t exist')\r\n",
        "        print(f'Trained checkpoint directory from {str(Path(prm.trained_checkpoint_dir).resolve())}')\r\n",
        "        if (not os.path.exists(prm.output_directory)):\r\n",
        "            print('Creating the output directory')\r\n",
        "            os.mkdir(prm.output_directory)\r\n",
        "        if (str(Path(prm.output_directory).resolve()) == str(Path(prm.model_dir).resolve())):\r\n",
        "            raise Exception(\"Error: export directory cannot be the train directory\")\r\n",
        "        print(f'The trained model will be in {str(Path(prm.model_dir).resolve())}')\r\n",
        "    # Copy the label file in the export directory\r\n",
        "    shutil.copy2(os.path.join(prm.trained_checkpoint_dir, 'label_map.pbtxt'), prm.output_directory)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    prm = ('prm' in locals() and isinstance(prm, ExportParameters) and prm) or ExportParameters.default\r\n",
        "    init_export_environment(prm)\r\n",
        "\r\n",
        "#@markdown ---\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWArB18vuDu9",
        "cellView": "form"
      },
      "source": [
        "# Module: export_main.py\r\n",
        "#@title #Export { form-width: \"20%\" }\r\n",
        "#@markdown The main train loop. It trains the model and put it in the output directory.\r\n",
        "#@markdown \r\n",
        "#@markdown It can be stopped before the completion when\r\n",
        "#@markdown a considerable result is reached and restart after for enhancing the tuning.\r\n",
        "\r\n",
        "from    absl import flags\r\n",
        "import  os\r\n",
        "import  sys\r\n",
        "\r\n",
        "try:    from    utilities import *\r\n",
        "except: pass\r\n",
        "\r\n",
        "# Avoiding the absl error for duplicated flags if run again the cell from a notebook\r\n",
        "for f in flags.FLAGS.flag_values_dict():\r\n",
        "    flags.FLAGS[f].allow_override = True\r\n",
        "\r\n",
        "def export_main(unused_argv):\r\n",
        "    # Part of code not executed on Colab notebook\r\n",
        "    def run_py_mode():\r\n",
        "        # Init the train environment\r\n",
        "        from export_environment import init_export_environment\r\n",
        "        from export_parameters import ExportParameters\r\n",
        "        export_parameters = ExportParameters()\r\n",
        "        export_parameters.update_values()\r\n",
        "        init_export_environment(export_parameters)\r\n",
        "        # Import the export main function\r\n",
        "        from object_detection import exporter_main_v2\r\n",
        "        export_parameters.update_flags()\r\n",
        "        # Export the model\r\n",
        "        exporter_main_v2.main(unused_argv)\r\n",
        "    def run_notebook_mode():\r\n",
        "        # Import the train main function\r\n",
        "        from object_detection import exporter_main_v2\r\n",
        "        prm.update_flags()\r\n",
        "        # Execute the train\r\n",
        "        exporter_main_v2.main(unused_argv)\r\n",
        "    # Execution\r\n",
        "    if (is_jupyter()):\r\n",
        "        run_notebook_mode()\r\n",
        "    else:\r\n",
        "        run_py_mode()\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    if (not is_jupyter()):\r\n",
        "        from od_install import install_object_detection\r\n",
        "        install_object_detection()\r\n",
        "    import tensorflow as tf\r\n",
        "    try:\r\n",
        "        tf.compat.v1.app.run(export_main)\r\n",
        "    except SystemExit:\r\n",
        "        pass\r\n",
        "    print('Export complete')\r\n",
        "\r\n",
        "#@markdown ---\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}