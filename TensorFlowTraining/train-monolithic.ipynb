{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oGKniD-fesc"
      },
      "source": [
        "# Object detection train with TensorFlow 2.4.1\r\n",
        "This notebook will train a model for the object detection purpouse.\r\n",
        "It can be run as a Jupiter notebook in the Google Colab environment or exported as a Python file and run from a command line.\r\n",
        "# Basic Notebook usage:\r\n",
        "**model_type:**\r\n",
        "\r\n",
        ">The chosen base pre-trained model type (from TensorFlow 2 model zoo). Write it between quotation marks\r\n",
        "The list of available models are:\r\n",
        "\r\n",
        "*   SSD MobileNet v2 320x320\r\n",
        "*   SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\r\n",
        "\r\n",
        "**model_dir:**\r\n",
        "> the output directry for the trained models' checkpoints.\r\n",
        "\r\n",
        "**train_images_dir:**\r\n",
        "> the directory on your Google Drive containing the images for the train and their xml annotations. You can use standard images annotation tools as [labelImg](https://github.com/tzutalin/labelImg), [tVoTT](https://github.com/microsoft/VoTT), etc...\r\n",
        "Put the xml generated files in te same directory of the images.\r\n",
        "\r\n",
        "**eval_images_dir:**\r\n",
        "> the directory on your Google Drive containing the images used for evaluating the train. They could be near from 10% to 20% of the number of the train images. They must be labeled as the train images.\r\n",
        "\r\n",
        "For other parameters see the comments in the below code.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE1vVxUaQycS"
      },
      "source": [
        "# =============================================================================\r\n",
        "# For local training/evaluation run for example:\r\n",
        "# python TensorFlowTraining.py \\\r\n",
        "#     --model_dir=TrainedModel\r\n",
        "#     --train_images_dir=Images/Train\r\n",
        "#     --eval_images_dir=Images/Eval\r\n",
        "#     --model_type=\"SSD MobileNet v2 320x320\"\r\n",
        "#     --num_train_steps=10000\r\n",
        "#     --alsologtostderr\r\n",
        "# =============================================================================\r\n",
        "# The available models are:\r\n",
        "# - SSD MobileNet v2 320x320\r\n",
        "# - SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\r\n",
        "# =============================================================================\r\n",
        "# Globals\r\n",
        "# =============================================================================\r\n",
        "# The type of the base model\r\n",
        "model_type = \"SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\"\r\n",
        "# Training model output directory\r\n",
        "model_dir = \"trained-model\"\r\n",
        "# Directory containing the images for the training\r\n",
        "train_images_dir = \"images/train\"\r\n",
        "# Directory containing the images for the evaluation\r\n",
        "eval_images_dir = \"images/eval\"\r\n",
        "# =============================================================================\r\n",
        "# Arguments\r\n",
        "# =============================================================================\r\n",
        "from absl import flags\r\n",
        "flags.DEFINE_string('model_type', None, 'Type of the base model.')\r\n",
        "flags.DEFINE_string('model_dir', None, 'Path to output model directory '\r\n",
        "                    'where event and checkpoint files will be written.')\r\n",
        "flags.DEFINE_string('train_images_dir', None, 'Path to the directory '\r\n",
        "                    'containing the images for train and their labeling xml.')\r\n",
        "flags.DEFINE_string('eval_images_dir', None, 'Path to the directory '\r\n",
        "                    'containing the images for evaluate and their labeling xml.')\r\n",
        "flags.DEFINE_integer('num_train_steps', None, 'Number of train steps.')\r\n",
        "flags.DEFINE_bool('eval_on_train_data', False, 'Enable evaluating on train '\r\n",
        "                  'data (only supported in distributed training).')\r\n",
        "flags.DEFINE_integer('sample_1_of_n_eval_examples', None, 'Will sample one of '\r\n",
        "                     'every n eval input examples, where n is provided.')\r\n",
        "flags.DEFINE_integer('sample_1_of_n_eval_on_train_examples', 5, 'Will sample '\r\n",
        "                     'one of every n train input examples for evaluation, '\r\n",
        "                     'where n is provided. This is only used if '\r\n",
        "                     '`eval_training_data` is True.')\r\n",
        "flags.DEFINE_string('checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '\r\n",
        "                    '`checkpoint_dir` is provided, this binary operates in eval-only mode, '\r\n",
        "                    'writing resulting metrics to `model_dir`.')\r\n",
        "flags.DEFINE_integer('eval_timeout', 3600, 'Number of seconds to wait for an'\r\n",
        "                     'evaluation checkpoint before exiting.')\r\n",
        "flags.DEFINE_bool('use_tpu', False, 'Whether the job is executing on a TPU.')\r\n",
        "flags.DEFINE_string('tpu_name', default=None, help='Name of the Cloud TPU for Cluster Resolvers.')\r\n",
        "flags.DEFINE_integer('num_workers', 1, 'When num_workers > 1, training uses '\r\n",
        "                      'MultiWorkerMirroredStrategy. When num_workers = 1 it uses '\r\n",
        "                    'MirroredStrategy.')\r\n",
        "flags.DEFINE_integer('checkpoint_every_n', 1000, 'Integer defining how often we checkpoint.')\r\n",
        "flags.DEFINE_boolean('record_summaries', True,\r\n",
        "                     ('Whether or not to record summaries during'\r\n",
        "                      ' training.'))\r\n",
        "FLAGS = flags.FLAGS\r\n",
        "# =============================================================================\r\n",
        "# List of available models and theirs configurations\r\n",
        "# =============================================================================\r\n",
        "models = {\r\n",
        "    \"SSD MobileNet v2 320x320\": {\r\n",
        "        \"DownloadPath\": \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\",\r\n",
        "        \"batch_size\": 12,\r\n",
        "        \"height\": 300,\r\n",
        "        \"width\": 300\r\n",
        "    },\r\n",
        "    \"SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\": {\r\n",
        "        \"DownloadPath\": \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\",\r\n",
        "        \"batch_size\": 8,\r\n",
        "        \"height\": 640,\r\n",
        "        \"width\": 640\r\n",
        "    },\r\n",
        "}\r\n",
        "model = models[model_type]\r\n",
        "annotations_dir = \"annotations\"\r\n",
        "# =============================================================================\r\n",
        "# Standard imports\r\n",
        "# =============================================================================\r\n",
        "import os\r\n",
        "import glob\r\n",
        "from pathlib import Path\r\n",
        "import shutil\r\n",
        "import subprocess\r\n",
        "import sys\r\n",
        "import tempfile\r\n",
        "# =============================================================================\r\n",
        "# Processes execution with printing\r\n",
        "# =============================================================================\r\n",
        "def executeSubprocess(cmd):\r\n",
        "    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, universal_newlines=True)\r\n",
        "    for stdout_line in iter(popen.stdout.readline, \"\"):\r\n",
        "        yield stdout_line \r\n",
        "    popen.stdout.close()\r\n",
        "    return_code = popen.wait()\r\n",
        "    if return_code:\r\n",
        "        raise subprocess.CalledProcessError(return_code, cmd)\r\n",
        "def execute(cmd):\r\n",
        "    for output in executeSubprocess(cmd):\r\n",
        "        print(output, end=\"\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eECHryVmTLg"
      },
      "source": [
        "# =============================================================================\r\n",
        "# Well known environment and dependencies installation\r\n",
        "# =============================================================================\r\n",
        "def InstallDependencies():\r\n",
        "    # Path of the python interpreter executable\r\n",
        "    pythonPath = os.path.join(os.path.dirname(sys.executable), \"python3\")\r\n",
        "    if (not os.path.exists(pythonPath)):\r\n",
        "        pythonPath = os.path.join(os.path.dirname(sys.executable), \"python\")\r\n",
        "    # Upgrade pip and setuptools\r\n",
        "    execute([pythonPath, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip==21.0.1\"])\r\n",
        "    execute([pythonPath, \"-m\", \"pip\", \"install\", \"--upgrade\", \"setuptools==54.0.0\"])\r\n",
        "    # Install TensorFlow\r\n",
        "    execute([pythonPath, \"-m\", \"pip\", \"install\", \"tensorflow==2.4.1\"])\r\n",
        "    # Install pygit2\r\n",
        "    execute([pythonPath, \"-m\", \"pip\", \"install\", \"pygit2==1.5.0\"])\r\n",
        "    # Progress class for the git output\r\n",
        "    import pygit2\r\n",
        "    import datetime\r\n",
        "    class GitCallbacks(pygit2.RemoteCallbacks):\r\n",
        "        def __init__(self, credentials=None, certificate=None):\r\n",
        "            self.dateTime = datetime.datetime.now()\r\n",
        "            return super().__init__(credentials=credentials, certificate=certificate)\r\n",
        "        def transfer_progress(self, stats):\r\n",
        "            now = datetime.datetime.now()\r\n",
        "            if ((now - self.dateTime).total_seconds() > 1):\r\n",
        "                print(\"\\rReceiving... Deltas [%d / %d], Objects [%d / %d]\"%(stats.indexed_deltas, stats.total_deltas, stats.indexed_objects, stats.total_objects), end=\"\", flush=True)\r\n",
        "                self.dateTime = now\r\n",
        "            if (stats.received_objects >= stats.total_objects and stats.indexed_objects >= stats.total_objects and stats.indexed_deltas >= stats.total_deltas):\r\n",
        "                print(\"\\r\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\rDone Deltas %d, Objects %d.\"%(stats.total_objects, stats.total_objects))\r\n",
        "            return super().transfer_progress(stats)\r\n",
        "    # Directory of the TensorFlow object detection api\r\n",
        "    odApiDir = os.path.join(tempfile.gettempdir(), \"tensorflow-object-detection-api\")\r\n",
        "    # Install the TensorFlow models\r\n",
        "    if (not os.path.isdir(odApiDir)):\r\n",
        "        # Create the callback for the progress\r\n",
        "        callbacks = GitCallbacks();\r\n",
        "        # Clone the TensorFlow models repository\r\n",
        "        print(\"Cloning the TensorFlow models repository\")\r\n",
        "        pygit2.clone_repository(\"https://github.com/tensorflow/models.git\", odApiDir, callbacks = callbacks)\r\n",
        "        print(\"TensorFlow models repository cloned\")\r\n",
        "        # Checkout a well known commit\r\n",
        "        repo = pygit2.Repository(odApiDir)\r\n",
        "        ish = \"e356598a5b79a768942168b10d9c1acaa923bdb4\"\r\n",
        "        (commit, reference) = repo.resolve_refish(ish)\r\n",
        "        repo.checkout_tree(commit)\r\n",
        "        repo.reset(pygit2.Oid(hex=ish), pygit2.GIT_RESET_HARD)\r\n",
        "        # Move to the research dir\r\n",
        "        currentDir = os.getcwd()\r\n",
        "        os.chdir(os.path.join(odApiDir, \"research\"))\r\n",
        "        # Install the protobuf tools\r\n",
        "        execute([pythonPath, \"-m\", \"pip\", \"install\", \"grpcio-tools==1.32.0\"])\r\n",
        "        # Compile the protobufs\r\n",
        "        import grpc_tools.protoc as protoc\r\n",
        "        protoFiles = Path(\"object_detection/protos\").rglob(\"*.proto\")\r\n",
        "        for protoFile in protoFiles:\r\n",
        "            protoFilePath = str(protoFile)\r\n",
        "            print(\"Compiling\", protoFilePath)\r\n",
        "            protoc.main([\"grpc_tools.protoc\", \"--python_out=.\", protoFilePath])\r\n",
        "        # Install the object detection packages\r\n",
        "        shutil.copy2(\"object_detection/packages/tf2/setup.py\", \".\")\r\n",
        "        execute([pythonPath, \"-m\", \"pip\", \"install\", \".\"])\r\n",
        "        os.chdir(currentDir)\r\n",
        "    sys.path.append(os.path.join(odApiDir, \"research\"))\r\n",
        "    sys.path.append(os.path.join(odApiDir, \"research/slim\"))\r\n",
        "    import tensorflow as tf\r\n",
        "    tf.config.set_soft_device_placement(True)\r\n",
        "    print(\"Installation completed.\")\r\n",
        "if ('google.colab' in sys.modules): InstallDependencies()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnOLYXFOmWsY"
      },
      "source": [
        "# =============================================================================\r\n",
        "# Directories mounting, checking and creating\r\n",
        "# =============================================================================\r\n",
        "def InitEnvironment():\r\n",
        "    global model_dir\r\n",
        "    global train_images_dir\r\n",
        "    global eval_images_dir\r\n",
        "    # Set the configuration for Google Colab\r\n",
        "    if ('google.colab' in sys.modules):\r\n",
        "        if (not os.path.exists(\"/mnt/MyDrive\")):\r\n",
        "            print(\"Mounting the GDrive\")\r\n",
        "            from google.colab import drive\r\n",
        "            drive.mount(\"/mnt\")\r\n",
        "        # Check the existence of the train images dir\r\n",
        "        gdriveOutputDir = os.path.join(\"/mnt\", \"MyDrive\", train_images_dir)\r\n",
        "        if (not os.path.isdir(gdriveOutputDir)):\r\n",
        "            print(\"Error!!! The train images dir doesn't exist\")\r\n",
        "            exit(-1)\r\n",
        "        if (os.path.exists(\"/content/train-images\")):\r\n",
        "            os.unlink(\"/content/train-images\")\r\n",
        "        os.symlink(gdriveOutputDir, \"/content/train-images\", True)\r\n",
        "        train_images_dir = \"/content/train-images\"\r\n",
        "        # Check the existence of the evaluation images dir\r\n",
        "        gdriveOutputDir = os.path.join(\"/mnt\", \"MyDrive\", eval_images_dir)\r\n",
        "        if (not os.path.isdir(gdriveOutputDir)):\r\n",
        "            print(\"Error!!! The evaluation images dir doesn't exist\")\r\n",
        "            exit(-1)\r\n",
        "        if (os.path.exists(\"/content/eval-images\")):\r\n",
        "            os.unlink(\"/content/eval-images\")\r\n",
        "        os.symlink(gdriveOutputDir, \"/content/eval-images\", True)\r\n",
        "        eval_images_dir = \"/content/eval-images\"\r\n",
        "        # Check the existence of the output directory\r\n",
        "        gdriveOutputDir = os.path.join(\"/mnt\", \"MyDrive\", model_dir)\r\n",
        "        if (not os.path.isdir(gdriveOutputDir)):\r\n",
        "            print(\"Creating the output directory\")\r\n",
        "            os.mkdir(gdriveOutputDir)\r\n",
        "        if (os.path.exists(\"/content/trained-model\")):\r\n",
        "            os.unlink(\"/content/trained-model\")\r\n",
        "        os.symlink(gdriveOutputDir, \"/content/trained-model\", True)\r\n",
        "        model_dir = \"/content/trained-model\"\r\n",
        "    else:\r\n",
        "        if (not os.path.isdir(train_images_dir)):\r\n",
        "            print(\"Error!!! The train images dir doesn't exist\")\r\n",
        "            exit(-1)\r\n",
        "        if (not os.path.isdir(eval_images_dir)):\r\n",
        "            print(\"Error!!! The evaluation images dir doesn't exist\")\r\n",
        "            exit(-1)\r\n",
        "        if (not os.path.exists(model_dir)):\r\n",
        "            print(\"Creating the output dir\")\r\n",
        "            os.mkdir(model_dir)\r\n",
        "    if (not os.path.exists(annotations_dir)):\r\n",
        "        os.mkdir(annotations_dir)\r\n",
        "if ('google.colab' in sys.modules): InitEnvironment()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax4Zkny6mZo3"
      },
      "source": [
        "# =============================================================================\r\n",
        "# Download the base model from the TensorFlow models zoo if it's needed\r\n",
        "# =============================================================================\r\n",
        "def DownloadPreTrainedModel():\r\n",
        "    import urllib.request\r\n",
        "    global preTrainedModelDir\r\n",
        "    # Set the directory where to download the pre-trained models\r\n",
        "    preTrainedModelBaseDir = os.path.join(tempfile.gettempdir(), \"tensorflow-pre-trained-models\")\r\n",
        "    preTrainedModelDir = str(Path(os.path.join(preTrainedModelBaseDir, Path(model[\"DownloadPath\"]).name)).with_suffix(\"\").with_suffix(\"\"))\r\n",
        "    if not (os.path.exists(preTrainedModelDir)):\r\n",
        "        if (not os.path.exists(preTrainedModelBaseDir)):\r\n",
        "            os.mkdir(preTrainedModelBaseDir)\r\n",
        "        preTrainedModelFile = preTrainedModelDir + \".tar.gz\"\r\n",
        "        print(f\"Downloading the pre-trained model {str(Path(preTrainedModelFile).name)}...\")\r\n",
        "        import tarfile\r\n",
        "        urllib.request.urlretrieve(model[\"DownloadPath\"], preTrainedModelFile) # TODO: show progress\r\n",
        "        print(\"Done.\")\r\n",
        "        print(f\"Extracting the pre-trained model {str(Path(preTrainedModelFile).name)}...\")\r\n",
        "        tar = tarfile.open(preTrainedModelFile)\r\n",
        "        tar.extractall(preTrainedModelBaseDir)\r\n",
        "        tar.close()\r\n",
        "        os.remove(preTrainedModelFile)\r\n",
        "        print(\"Done.\")\r\n",
        "if ('google.colab' in sys.modules): DownloadPreTrainedModel()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp9c2ufsmczW"
      },
      "source": [
        "# =============================================================================\r\n",
        "# Creation of the TFRecords\r\n",
        "# =============================================================================\r\n",
        "labelSet = set()\r\n",
        "labelDict = dict()\r\n",
        "# Convert the xml files generated by labeling image softwares into the cvs panda format\r\n",
        "def xml_to_csv(path):\r\n",
        "    import pandas as pd\r\n",
        "    import xml.etree.ElementTree as ET\r\n",
        "    xml_list = []\r\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\r\n",
        "        tree = ET.parse(xml_file)\r\n",
        "        root = tree.getroot()\r\n",
        "        for member in root.findall('object'):\r\n",
        "            value = (root.find('filename').text,\r\n",
        "                     int(root.find('size')[0].text),\r\n",
        "                     int(root.find('size')[1].text),\r\n",
        "                     member[0].text,\r\n",
        "                     int(member[4][0].text),\r\n",
        "                     int(member[4][1].text),\r\n",
        "                     int(member[4][2].text),\r\n",
        "                     int(member[4][3].text)\r\n",
        "                     )\r\n",
        "            xml_list.append(value)\r\n",
        "            labelSet.add(member[0].text)\r\n",
        "    column_name = ['filename', 'width', 'height',\r\n",
        "                   'class', 'xmin', 'ymin', 'xmax', 'ymax']\r\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\r\n",
        "    return xml_df\r\n",
        "# Convertion of the text of the labels to an integer index\r\n",
        "def class_text_to_int(row_label):\r\n",
        "    if (len(labelDict) == 0):\r\n",
        "        from google.protobuf import text_format\r\n",
        "        from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\r\n",
        "        count = len(labelSet)\r\n",
        "        labelIx = 1\r\n",
        "        for label in labelSet:\r\n",
        "            labelDict[label] = labelIx\r\n",
        "            labelIx += 1\r\n",
        "        msg = StringIntLabelMap()\r\n",
        "        for id, name in enumerate(labelSet, start=1):\r\n",
        "            msg.item.append(StringIntLabelMapItem(id=id, name=name))\r\n",
        "        text = str(text_format.MessageToBytes(msg, as_utf8=True), \"utf-8\")\r\n",
        "        global model_dir\r\n",
        "        with open(os.path.join(annotations_dir, \"label_map.pbtxt\"), \"w\") as f:\r\n",
        "            f.write(text)\r\n",
        "        with open(os.path.join(model_dir, \"label_map.pbtxt\"), \"w\") as f:\r\n",
        "            f.write(text)\r\n",
        "    return labelDict[row_label]\r\n",
        "# Splitting\r\n",
        "def split(df, group):\r\n",
        "    from collections import namedtuple\r\n",
        "    data = namedtuple('data', ['filename', 'object'])\r\n",
        "    gb = df.groupby(group)\r\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\r\n",
        "# TensorFlow example creator\r\n",
        "def create_tf_example(group, path):\r\n",
        "    from object_detection.utils import dataset_util\r\n",
        "    from PIL import Image\r\n",
        "    import io\r\n",
        "    import tensorflow as tf\r\n",
        "    with tf.compat.v1.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\r\n",
        "        encoded_jpg = fid.read()\r\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\r\n",
        "    image = Image.open(encoded_jpg_io)\r\n",
        "    width, height = image.size\r\n",
        "    filename = group.filename.encode('utf8')\r\n",
        "    image_format = b'jpg'\r\n",
        "    xmins = []\r\n",
        "    xmaxs = []\r\n",
        "    ymins = []\r\n",
        "    ymaxs = []\r\n",
        "    classes_text = []\r\n",
        "    classes = []\r\n",
        "    for index, row in group.object.iterrows():\r\n",
        "        xmins.append(row['xmin'] / width)\r\n",
        "        xmaxs.append(row['xmax'] / width)\r\n",
        "        ymins.append(row['ymin'] / height)\r\n",
        "        ymaxs.append(row['ymax'] / height)\r\n",
        "        classes_text.append(row['class'].encode('utf8'))\r\n",
        "        classes.append(class_text_to_int(row['class']))\r\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\r\n",
        "        'image/height': dataset_util.int64_feature(height),\r\n",
        "        'image/width': dataset_util.int64_feature(width),\r\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\r\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\r\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\r\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\r\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\r\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\r\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\r\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\r\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\r\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\r\n",
        "    }))\r\n",
        "    return tf_example\r\n",
        "# TensorFlow record file creator\r\n",
        "def create_tf_record(imageDir, outputFile, csvFile = None):\r\n",
        "    import tensorflow as tf\r\n",
        "    writer = tf.compat.v1.python_io.TFRecordWriter(outputFile)\r\n",
        "    path = os.path.join(imageDir)\r\n",
        "    examples = xml_to_csv(imageDir)\r\n",
        "    grouped = split(examples, 'filename')\r\n",
        "    for group in grouped:\r\n",
        "        tf_example = create_tf_example(group, path)\r\n",
        "        writer.write(tf_example.SerializeToString())\r\n",
        "    writer.close()\r\n",
        "    print('Successfully created the TFRecord file: {}'.format(outputFile))\r\n",
        "    if csvFile is not None:\r\n",
        "        examples.to_csv(csvFile, index=None)\r\n",
        "        print('Successfully created the CSV file: {}'.format(csvFile))\r\n",
        "# Create record files\r\n",
        "def CreateTFRecords():\r\n",
        "    global model_dir\r\n",
        "    global train_images_dir\r\n",
        "    global eval_images_dir\r\n",
        "    create_tf_record(train_images_dir, os.path.join(annotations_dir, \"train.record\"))\r\n",
        "    create_tf_record(eval_images_dir, os.path.join(annotations_dir, \"eval.record\"))\r\n",
        "if ('google.colab' in sys.modules): CreateTFRecords()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUCvqZlWmglm"
      },
      "source": [
        "# =============================================================================\r\n",
        "# Configuration of the training pipeline in the model output directory\r\n",
        "# =============================================================================\r\n",
        "def ConfigurePipeline():\r\n",
        "    import tensorflow as tf\r\n",
        "    from object_detection.protos import pipeline_pb2\r\n",
        "    from google.protobuf import text_format\r\n",
        "    global model_dir\r\n",
        "    global preTrainedModelDir\r\n",
        "    # Copy the pipeline configuration file if it's not already present in the output dir\r\n",
        "    print(\"Configuring the pipeline\")\r\n",
        "    outPipelineFile = os.path.join(model_dir, \"pipeline.config\")\r\n",
        "    shutil.copy2(os.path.join(preTrainedModelDir, \"pipeline.config\"), model_dir)\r\n",
        "    # Configuring the pipeline\r\n",
        "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\r\n",
        "    with tf.io.gfile.GFile(outPipelineFile, \"r\") as f:\r\n",
        "        proto_str = f.read()\r\n",
        "        text_format.Merge(proto_str, pipeline_config)\r\n",
        "    pipeline_config.model.ssd.num_classes = 1 # TODO define\r\n",
        "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = model[\"height\"]\r\n",
        "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = model[\"width\"]\r\n",
        "    pipeline_config.train_config.batch_size = model[\"batch_size\"]\r\n",
        "    pipeline_config.train_config.fine_tune_checkpoint = os.path.join(preTrainedModelDir, \"checkpoint\", \"ckpt-0\")\r\n",
        "    pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\r\n",
        "    pipeline_config.train_input_reader.label_map_path = os.path.join(annotations_dir, \"label_map.pbtxt\")\r\n",
        "    pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = os.path.join(annotations_dir, \"train.record\")\r\n",
        "    pipeline_config.eval_input_reader[0].label_map_path = os.path.join(annotations_dir, \"label_map.pbtxt\")\r\n",
        "    pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = os.path.join(annotations_dir, \"eval.record\")\r\n",
        "    config_text = text_format.MessageToString(pipeline_config)\r\n",
        "    with tf.io.gfile.GFile(outPipelineFile, \"wb\") as f:\r\n",
        "        f.write(config_text)\r\n",
        "    shutil.copy2(outPipelineFile, annotations_dir)\r\n",
        "    print(str(config_text))\r\n",
        "    print(\"Done.\")\r\n",
        "if ('google.colab' in sys.modules): ConfigurePipeline()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mi9oZq9mjHl"
      },
      "source": [
        "# =============================================================================\r\n",
        "# Main training function\r\n",
        "# =============================================================================\r\n",
        "def main(unused_argv):\r\n",
        "    global model_dir\r\n",
        "    global train_images_dir\r\n",
        "    global eval_images_dir\r\n",
        "    global model_type\r\n",
        "    global model\r\n",
        "    global models\r\n",
        "    from object_detection import model_lib_v2\r\n",
        "    #flags.mark_flag_as_required('model_dir')\r\n",
        "    #flags.mark_flag_as_required('pipeline_config_path')\r\n",
        "    if (FLAGS.model_dir != None): model_dir = FLAGS.model_dir\r\n",
        "    if (FLAGS.train_images_dir != None): train_images_dir=FLAGS.train_images_dir\r\n",
        "    if (FLAGS.eval_images_dir != None): eval_images_dir=FLAGS.eval_images_dir\r\n",
        "    if (FLAGS.model_type != None): model_type = FLAGS.model_type\r\n",
        "    model = models[model_type]\r\n",
        "    if (not 'google.colab' in sys.modules):\r\n",
        "        InitEnvironment()\r\n",
        "        DownloadPreTrainedModel()\r\n",
        "        CreateTFRecords()\r\n",
        "        ConfigurePipeline()\r\n",
        "    if FLAGS.checkpoint_dir:\r\n",
        "        model_lib_v2.eval_continuously(\r\n",
        "            pipeline_config_path=os.path.join(annotations_dir, \"pipeline.config\"),\r\n",
        "            model_dir=model_dir,\r\n",
        "            train_steps=FLAGS.num_train_steps,\r\n",
        "            sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\r\n",
        "            sample_1_of_n_eval_on_train_examples=(FLAGS.sample_1_of_n_eval_on_train_examples),\r\n",
        "            checkpoint_dir=FLAGS.checkpoint_dir,\r\n",
        "            wait_interval=300,\r\n",
        "            timeout=FLAGS.eval_timeout)\r\n",
        "    else:\r\n",
        "        if FLAGS.use_tpu:\r\n",
        "            # TPU is automatically inferred if tpu_name is None and\r\n",
        "            # we are running under cloud ai-platform.\r\n",
        "            resolver = tf.distribute.cluster_resolver.TPUClusterResolver(FLAGS.tpu_name)\r\n",
        "            tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "            tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n",
        "        elif FLAGS.num_workers > 1:\r\n",
        "            strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n",
        "        else:\r\n",
        "            strategy = tf.compat.v2.distribute.MirroredStrategy()\r\n",
        "    with strategy.scope():\r\n",
        "        model_lib_v2.train_loop(\r\n",
        "            pipeline_config_path=os.path.join(annotations_dir, \"pipeline.config\"),\r\n",
        "            model_dir=model_dir,\r\n",
        "            train_steps=FLAGS.num_train_steps,\r\n",
        "            use_tpu=FLAGS.use_tpu,\r\n",
        "            checkpoint_every_n=FLAGS.checkpoint_every_n,\r\n",
        "            record_summaries=FLAGS.record_summaries)\r\n",
        "# =============================================================================\r\n",
        "# Launch the main function for the training\r\n",
        "# =============================================================================\r\n",
        "if __name__ == '__main__':\r\n",
        "    if (not 'google.colab' in sys.modules):\r\n",
        "        InstallDependencies()\r\n",
        "    import tensorflow as tf\r\n",
        "    tf.compat.v1.app.run()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}